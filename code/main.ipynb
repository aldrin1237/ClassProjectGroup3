{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Necessities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# CLASS Project\n",
    "# PYTHON IMPLEMENTATION: Introduction to Deep Learning and Neural Networks\n",
    "# Course: CMPS3500\n",
    "# Date: 12/06/24\n",
    "# Student 1: Aldrin Amistoso\n",
    "# Student 2: Jesse Garcia\n",
    "# Student 3: Marc Angeles\n",
    "# Student 4: Marvin Estrada\n",
    "##############################################################\n",
    "\n",
    "# General Packages\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# data handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# visualization libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# extra libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Packages to support NN\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a folder called data within my project file\n",
    "#  .. my_project\n",
    "#     |\n",
    "#     |___code\n",
    "#     |   |\n",
    "#     |   |__ CS3500_Starter_Notebook.ipynb\n",
    "#     |\n",
    "#     |___data\n",
    "#         |\n",
    "#         |__ credit_score.csv\n",
    "#\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# # Get the current working directory\n",
    "# current_dir = os.getcwd() \n",
    "\n",
    "# # Construct a path to the parent directory\n",
    "# parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# # Access a file in the parent directory\n",
    "# file_path = os.path.join(parent_dir, \"data/credit_score_data.csv\")\n",
    "\n",
    "# # Load Credit Score data\n",
    "# df = pd.read_csv(file_path) \n",
    "\n",
    "# global variable to track state\n",
    "data_loaded = False\n",
    "data_cleaned = False\n",
    "model_trained = False\n",
    "\n",
    "def loadData():\n",
    "    global df, data_loaded\n",
    "    \"\"\"Load the credit score dataset and display summary statistics.\"\"\"\n",
    "    print(\"\\nLoading and cleaning input data set:\")\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd() \n",
    "\n",
    "    # Construct a path to the parent directory\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "    # Access a file in the parent directory\n",
    "    file_path = os.path.join(parent_dir, \"data/credit_score_data.csv\")\n",
    "    \n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting Script\")\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Loading training data set\")\n",
    "    \n",
    "    try:\n",
    "        # Load Credit Score data\n",
    "        df = pd.read_csv(file_path) \n",
    "        total_columns = df.shape[1]\n",
    "        total_rows = df.shape[0]\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Total Columns Read: {total_columns}\")\n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Total Rows Read: {total_rows}\")\n",
    "        data_loaded = True\n",
    "        print(f\"\\nTime to load is: {round(end_time - start_time, 2)} seconds\")\n",
    "        \n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file was not found. Please ensure the file path is correct and try again.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty. Please provide a valid dataset file and try again.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        print(\"\\nReturning to the main menu. Please try again.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions For Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_numerical_column(series, col_name):\n",
    "    '''Describe a numerical column using describe function of series, report number of null values, display boxplots and histograms.\n",
    "    Return min, max, IQR based outlier lower range and IQR based outlier upper range as a dictionary.'''\n",
    "    # print(series.describe(), end = '\\n\\n')\n",
    "\n",
    "    # print(f'Number of null values: {series.isnull().sum()}', '\\n\\n')\n",
    "    \n",
    "    # fig, ax = plt.subplots(2, 1, figsize = (8, 8), sharex = True)\n",
    "    # sns.boxplot(series, orient = 'h', ax = ax[0])\n",
    "    # ax[0].set_title(f'Distribution of {col_name}')\n",
    "    # ax[0].tick_params(left = False, labelleft = False) \n",
    "    # sns.histplot(series, ax = ax[1])\n",
    "    # ax[1].set_ylabel('Frequency')\n",
    "    # ax[1].set_xlabel(col_name)\n",
    "    # plt.show();\n",
    "\n",
    "    q1, q3 = series.quantile([0.25, 0.75])\n",
    "    IQR = q3 - q1\n",
    "    \n",
    "    return  {'Min. value': series.min(), 'Outlier lower range': q1 - 1.5 * IQR, 'Outlier upper range': q3 + 1.5 * IQR, 'Max. value': series.max()}\n",
    "\n",
    "def summarize_numerical_column_with_deviation(data, num_col, group_col = 'Customer_ID', absolute_summary = True, median_standardization_summary = False):\n",
    "    '''Summarize the numerical column and its median standardization based on customers using describe_numerical_column function.'''\n",
    "    Summary_dict = {}\n",
    "    \n",
    "    if absolute_summary == True:\n",
    "        # print(f'Column description for {num_col}:\\n')\n",
    "        Summary_dict[num_col] = describe_numerical_column(data[num_col], num_col)\n",
    "        \n",
    "    if median_standardization_summary == True:\n",
    "        # if absolute_summary == True:\n",
    "        #     print('\\n')\n",
    "        default_MAD = return_max_MAD(data, num_col, group_col)\n",
    "        num_col_standardization = data.groupby(group_col)[num_col].apply(median_standardization, default_value = default_MAD)\n",
    "        # print(f'Median standardization for {num_col}:\\n')\n",
    "        Summary_dict[f'Median standardization of {num_col}'] = describe_numerical_column(num_col_standardization, f'Median standardization of {num_col}')\n",
    "        Summary_dict['Max. MAD'] = default_MAD\n",
    "    return Summary_dict\n",
    "\n",
    "def return_max_MAD(data, num_col, group_col = 'Customer_ID'):\n",
    "    '''Return max value of median absolute devaition(MAD) from within the customers for num_col'''\n",
    "    return (data.groupby(group_col)[num_col].agg(lambda x: (x - x.median()).abs().median())).max()\n",
    "    \n",
    "def validate_age(x):\n",
    "    '''Check whether 8-months period age for a customer is logically valid or not'''\n",
    "    diff = x.diff()\n",
    "    if (diff == 0).sum() == 7:\n",
    "        return True\n",
    "    elif ((diff.isin([0, 1])).sum() == 7) and ((diff == 1).sum() == 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "def median_standardization(x, default_value):\n",
    "    '''Transform series or dataframe to its devaition from median with respect to Median absolute deviation(MAD) i.e. median standardization.'''\n",
    "    med = x.median() \n",
    "    abs = (x - med).abs()\n",
    "    MAD = abs.median()\n",
    "    if MAD == 0:\n",
    "        if ((abs == 0).sum() == abs.notnull().sum()): # When MAD is zero and all non-null values are constant in x\n",
    "            return x * 0\n",
    "        else:\n",
    "            return (x - med)/default_value # When MAD is zero but all non-values are not same in x\n",
    "    else:\n",
    "        return (x - med)/MAD # When MAD is non-zero\n",
    "\n",
    "def return_num_of_modes(x):\n",
    "    '''Return number of modes in given series or dataframe'''\n",
    "    return len(x.mode())\n",
    "\n",
    "def return_mode(x):\n",
    "    '''Return nan if no mode exists in given series or return minimum mode'''\n",
    "    modes = x.mode()\n",
    "    if len(modes) == 0:\n",
    "        return np.nan\n",
    "    return modes.min()\n",
    "\n",
    "def forward_backward_fill(x):\n",
    "    '''Perform forward fill then backward fill on given series or dataframe'''\n",
    "    return x.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "def return_mode_median_filled_int(x):\n",
    "    '''Return back series by filling with mode(in case there is one mode) else fill with integer part of median'''\n",
    "    modes = x.mode()\n",
    "    if len(modes) == 1:\n",
    "        return x.fillna(modes[0])\n",
    "    else:\n",
    "        return x.fillna(int(modes.median()))\n",
    "\n",
    "def return_mode_average_filled(x):\n",
    "    '''Return back series by filling with mode(in case there is one mode) else fill with average of modes'''\n",
    "    modes = x.mode()\n",
    "    if len(modes) == 1:\n",
    "        return x.fillna(modes[0])\n",
    "    else:\n",
    "        return x.fillna(modes.mean())\n",
    "\n",
    "def fill_month_history(x):\n",
    "    '''Return months filled data for 8-months period'''\n",
    "    first_non_null_idx = x.argmin()\n",
    "    first_non_null_value = x.iloc[first_non_null_idx]\n",
    "    return pd.Series(first_non_null_value + np.array(range(-first_non_null_idx, 8-first_non_null_idx)), index = x.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions For Neural Networks Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate predicted vs test data categorical variables\n",
    "def plot_prediction_vs_test_categorical(y_test, y_pred, class_labels):\n",
    "    # Plots the prediction vs test data for categorical variables.\n",
    "\n",
    "    # Args:\n",
    "    #     y_test (array-like): True labels of the test data.\n",
    "    #     y_pred (array-like): Predicted labels of the test data.\n",
    "    #     class_labels (list): List of class labels.\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Calculates performance of multivariate classification model\n",
    "def calculate_performance_multiclass(y_true, y_pred):\n",
    "    # Calculates various performance metrics for multiclass classification.\n",
    "\n",
    "    # Args:\n",
    "    #     y_true: The true labels.\n",
    "    #     y_pred: The predicted labels.\n",
    "\n",
    "    # Returns:\n",
    "    #     A dictionary containing the calculated metrics.\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Accuracy\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Precision, Recall, and F1-score (macro-averaged)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "    metrics['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns, data cleaning and correcting data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Ideally data cleaning should be done in parallel to discussions with domain expert to understand what values are appropriate in the columns, can they be retrieved if missing and do the columns depend upon each other. Unfortunately, such kind of support is not available in this kaggle project and therefore, we will deal with the data as per our understanding approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dataset info, many of the columns in our dataset have null values within them, representing missing values. Also, some columns are not of the correct data type as per the data they hold, this means there might be some textual characters within the data indicating unclean data and maybe placeholders which describe non-existing data or missing data and therefore, that is not getting captured as null values but as strings. We need to identify these values and first change them to null values before we do any further pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the columns one by one. Only columns which need some cleaning will be dealt with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaning():\n",
    "    global df, data_loaded, data_cleaned\n",
    "    try:\n",
    "        if not data_loaded: # Check if data is loaded\n",
    "            raise ValueError(\"Data has not been loaded. Please load the data first using option (1).\")\n",
    "\n",
    "        print(\"Process (Clean) data:\")\n",
    "        print(\"*********************\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Cleaning start\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Performing Data Clean Up\")\n",
    "\n",
    "        # #### 1. Customer ID\n",
    "\n",
    "        # %%\n",
    "        df['Customer_ID'].unique()\n",
    "\n",
    "        # %%\n",
    "        df['Customer_ID'].nunique()\n",
    "\n",
    "        df['Customer_ID'].str.contains('CUS_0x').value_counts()\n",
    "\n",
    "        # #### 2. Name\n",
    "        df.drop(columns = ['Name'], inplace = True)\n",
    "\n",
    "        # #### 3. Age\n",
    "        df['Age'][~df['Age'].str.isnumeric()].unique() #extracting non-numeric textual data\n",
    "        df['Age'] = df['Age'].str.replace('_', '')\n",
    "        df['Age'][~df['Age'].str.isnumeric()].unique()\n",
    "        df['Age'] = df['Age'].astype(int)\n",
    "\n",
    "        # #### 4. SSN\n",
    "        df.drop(columns = ['SSN'], inplace = True)\n",
    "\n",
    "        # #### 5. Occupation\n",
    "        df['Occupation'][df['Occupation'] == '_______'] = np.nan\n",
    "\n",
    "        # #### 6. Annual Income\n",
    "        df['Annual_Income'][~df['Annual_Income'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique() # using regex to find values which don't follow the patern of a float\n",
    "        df['Annual_Income'] = df['Annual_Income'].str.replace('_', '')\n",
    "        df['Annual_Income'][~df['Annual_Income'].str.fullmatch('([0-9]*[.])?[0-9]+')]\n",
    "        df['Annual_Income'] = df['Annual_Income'].astype(float)\n",
    "\n",
    "        # #### 7. Number of Loans\n",
    "\n",
    "        df['Num_of_Loan'][~df['Num_of_Loan'].str.isnumeric()].unique()\n",
    "        df['Num_of_Loan'] = df['Num_of_Loan'].str.replace('_', '').astype(int)\n",
    "\n",
    "        # #### 8. Number of delayed payments\n",
    "        temp_series = df['Num_of_Delayed_Payment'][df['Num_of_Delayed_Payment'].notnull()]\n",
    "\n",
    "        temp_series[~temp_series.str.isnumeric()].unique()\n",
    "        df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].str.replace('_', '').astype(float)\n",
    "\n",
    "        # #### 9. Changed Credit Limit\n",
    "        df['Changed_Credit_Limit'][~df['Changed_Credit_Limit'].str.fullmatch('[+-]?([0-9]*[.])?[0-9]+')].unique()\n",
    "        df['Changed_Credit_Limit'][df['Changed_Credit_Limit'] == '_'] = np.nan \n",
    "        df['Changed_Credit_Limit'] = df['Changed_Credit_Limit'].astype(float)\n",
    "\n",
    "        # #### 10. Credit Mix\n",
    "        df['Credit_Mix'][df['Credit_Mix'] == '_'] = np.nan\n",
    "\n",
    "        # #### 11. Outstanding debt\n",
    "        df['Outstanding_Debt'][~df['Outstanding_Debt'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique()\n",
    "        df['Outstanding_Debt'] = df['Outstanding_Debt'].str.replace('_', '')\n",
    "        df['Outstanding_Debt'][~df['Outstanding_Debt'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique()\n",
    "        df['Outstanding_Debt'] = df['Outstanding_Debt'].astype(float)\n",
    "        \n",
    "        # #### 12. Amount Invested Monthly\n",
    "        temp_series = df['Amount_invested_monthly'][df['Amount_invested_monthly'].notnull()]\n",
    "        temp_series[~temp_series.str.fullmatch('([0-9]*[.])?[0-9]+')].unique()\n",
    "        df['Amount_invested_monthly'] = df['Amount_invested_monthly'].str.replace('_', '').astype(float)\n",
    "\n",
    "        # #### 13. Payment Behaviour\n",
    "        df['Payment_Behaviour'][df['Payment_Behaviour'] == '!@9#%8'] = np.nan\n",
    "\n",
    "        # #### 14. Monthly Balance\n",
    "        temp_series = df['Monthly_Balance'][df['Monthly_Balance'].notnull()]\n",
    "        temp_series[temp_series.str.fullmatch('[+-]*([0-9]*[.])?[0-9]+') == False].unique()\n",
    "        df['Monthly_Balance'][df['Monthly_Balance'] == '__-333333333333333333333333333__'] = np.nan\n",
    "        df['Monthly_Balance'] = df['Monthly_Balance'].astype(float)\n",
    "        df['Month'] = df['Month'].map({'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6, 'July': 7, 'August': 8})\n",
    "        df.sort_values(by = ['Customer_ID', 'Month'], ignore_index = True, inplace = True)\n",
    "        df.drop(columns = 'ID', inplace = True)\n",
    "        df.head(8)\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # #### 1. Age \n",
    "        df['Age'][(df['Age'] > 100) | (df['Age'] <= 0)] = np.nan \n",
    "        summary_age = summarize_numerical_column_with_deviation(df, 'Age', median_standardization_summary = True)\n",
    "        df['Age'][df.groupby('Customer_ID')['Age'].transform(median_standardization, default_value = return_max_MAD(df, 'Age')) > 80] = np.nan\n",
    "        df['Age'] =  df.groupby('Customer_ID')['Age'].transform(forward_backward_fill).astype(int)\n",
    "        df.groupby('Customer_ID')['Age'].nunique().value_counts()\n",
    "\n",
    "        df.groupby('Customer_ID')['Age'].agg(validate_age).value_counts()\n",
    "\n",
    "        # #### 2. Occupation \n",
    "        df['Occupation'].isnull().sum()\n",
    "        df.groupby('Customer_ID')['Occupation'].nunique().value_counts()\n",
    "        df.groupby('Customer_ID')['Occupation'].count().value_counts()\n",
    "        df['Occupation'] = df.groupby('Customer_ID')['Occupation'].transform(forward_backward_fill)\n",
    "        df['Occupation'].isnull().sum()\n",
    "\n",
    "        # #### 3. Annual Income and monthly inhand salary\n",
    "        summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)\n",
    "\n",
    "        summary_monthly_inhand_salary = summarize_numerical_column_with_deviation(df, 'Monthly_Inhand_Salary', 'Customer_ID', True, True)\n",
    "\n",
    "        df.groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_num_of_modes).value_counts()\n",
    "        df[df.groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_num_of_modes) == 2]\n",
    "        df['Annual_Income'][df['Monthly_Inhand_Salary'].notnull()] = df[df['Monthly_Inhand_Salary'].notnull()].groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_mode)\n",
    "\n",
    "        summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)\n",
    "\n",
    "        df['Monthly_Inhand_Salary'] = df.groupby(['Customer_ID', 'Annual_Income'], group_keys = False)['Monthly_Inhand_Salary'].transform(forward_backward_fill)\n",
    "        df['Monthly_Inhand_Salary'].isnull().sum()\n",
    "        Annual_Income_deviation = df.groupby('Customer_ID', group_keys = False)['Annual_Income'].apply(lambda x: (x - x.median())/x.median())\n",
    "        temp = Annual_Income_deviation[df['Monthly_Inhand_Salary'].isnull()]\n",
    "\n",
    "        # print(temp.describe())\n",
    "        df['Annual_Income'][df['Monthly_Inhand_Salary'].isnull()] = np.nan\n",
    "        Annual_Income_deviation = df.groupby('Customer_ID', group_keys = False)['Annual_Income'].apply(lambda x: (x - x.median())/x.median())\n",
    "        Annual_Income_deviation[Annual_Income_deviation > 500]\n",
    "        df.iloc[[34042]]\n",
    "        df[df['Customer_ID'].isin(['CUS_0x6079'])]\n",
    "\n",
    "        df.loc[[34042], ['Annual_Income', 'Monthly_Inhand_Salary']] = np.nan\n",
    "\n",
    "        df['Annual_Income'] = df.groupby('Customer_ID')['Annual_Income'].transform(forward_backward_fill)\n",
    "        df['Monthly_Inhand_Salary'] = df.groupby('Customer_ID')['Monthly_Inhand_Salary'].transform(forward_backward_fill)\n",
    "        summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)\n",
    "        summary_monthly_inhand_salary = summarize_numerical_column_with_deviation(df, 'Monthly_Inhand_Salary', 'Customer_ID', True, False)\n",
    "        \n",
    "        # #### 4. Number of Bank Accounts\n",
    "        summary_num_bank_accounts = summarize_numerical_column_with_deviation(df, 'Num_Bank_Accounts', median_standardization_summary = True)\n",
    "        summary_num_bank_accounts\n",
    "\n",
    "        df['Num_Bank_Accounts'][df['Num_Bank_Accounts'] < 0] = np.nan\n",
    "        df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts')).value_counts()\n",
    "        np.sort((df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts'))).unique())[:10]\n",
    "        df['Num_Bank_Accounts'][df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts')).abs() > 2] = np.nan\n",
    "        summary_num_bank_accounts = summarize_numerical_column_with_deviation(df, 'Num_Bank_Accounts', median_standardization_summary = True)\n",
    "        df['Num_Bank_Accounts'] = df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(forward_backward_fill).astype(int)\n",
    "        df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "        df.groupby('Customer_ID')['Num_Bank_Accounts'].agg(lambda x: x.diff().sum()).value_counts()\n",
    "\n",
    "        # #### 5. Number of credit cards\n",
    "        summary_num_credit_cards = summarize_numerical_column_with_deviation(df, 'Num_Credit_Card', median_standardization_summary = True)\n",
    "        summary_num_credit_cards\n",
    "        df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card')).value_counts()\n",
    "        np.sort((df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card'))).unique())[:10]\n",
    "\n",
    "        df['Num_Credit_Card'][df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card')).abs() > 2] = np.nan\n",
    "\n",
    "        summary_num_credit_cards = summarize_numerical_column_with_deviation(df, 'Num_Credit_Card', median_standardization_summary = True)\n",
    "        df['Num_Credit_Card'] = df.groupby('Customer_ID')['Num_Credit_Card'].transform(forward_backward_fill).astype(int)\n",
    "        df.groupby('Customer_ID')['Num_Credit_Card'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "        df.groupby('Customer_ID')['Num_Credit_Card'].agg(lambda x: x.diff().sum()).value_counts()\n",
    "\n",
    "        # #### 6. Interest Rate\n",
    "        summary_interest_rate = summarize_numerical_column_with_deviation(df, 'Interest_Rate', median_standardization_summary = True)\n",
    "        summary_interest_rate\n",
    "        df.groupby('Customer_ID')['Interest_Rate'].nunique().value_counts()\n",
    "        # What we observe is MAD is 0(since max. MAD is 0) for each customer. Thus, it is hard to look at median standardization and assess points using this. Lets try to look at deviation from median. Since interest rate is not a feature whose median should deviate too much in scale from customer to customer.\n",
    "        deviation_from_median = df.groupby('Customer_ID')['Interest_Rate'].transform(lambda x: (x - x.median()))\n",
    "        deviation_from_median.describe()\n",
    "        deviation_from_median.value_counts()\n",
    "        np.sort(deviation_from_median.unique())\n",
    "        df['Interest_Rate'] = df.groupby('Customer_ID')['Interest_Rate'].transform(lambda x: x.median())\n",
    "        df['Interest_Rate'].describe()\n",
    "\n",
    "        # #### 7. Number of loans\n",
    "        summary_num_of_loans = summarize_numerical_column_with_deviation(df, 'Num_of_Loan')\n",
    "        df['Type_of_Loan'].isnull().sum()\n",
    "        num_of_loans = df['Type_of_Loan'].str.split(', ').str.len()\n",
    "        df['Num_of_Loan'][num_of_loans.notnull()] = num_of_loans[num_of_loans.notnull()]\n",
    "        df['Num_of_Loan'][num_of_loans.isnull()].value_counts()\n",
    "\n",
    "        # %%\n",
    "        np.sort(df['Num_of_Loan'][num_of_loans.isnull()].value_counts().index)\n",
    "        df['Num_of_Loan'][num_of_loans.isnull()] = 0\n",
    "\n",
    "        # %%\n",
    "        df['Num_of_Loan'] = df.groupby('Customer_ID')['Num_of_Loan'].transform(forward_backward_fill).astype(int)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # What if we take one level difference at customer level? The difference can be 0, negative or positive but shouldn't be too high.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Num_of_Loan'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # This means that number of loans remain same throughout the 8-months period for each customer.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 8. Type of loan\n",
    "\n",
    "        # %%\n",
    "        df['Type_of_Loan'].value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Here we see that the placeholder 'Not Specified' has been used as a way of indicating that the type of loan has not been specified by the customer. \n",
    "\n",
    "        # %%\n",
    "        df['Type_of_Loan'].nunique()\n",
    "\n",
    "        # %%\n",
    "        df['Type_of_Loan'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Total 11408 null values. As noted earlier with number of loans these most probably represent no loans.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # We can replace the same with our own placeholder for that - 'No Loan'.\n",
    "\n",
    "        # %%\n",
    "        df['Type_of_Loan'].fillna('No Loan', inplace = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Lets seen what and how many unique type of loans we have.\n",
    "\n",
    "        # %%\n",
    "        temp_series = df['Type_of_Loan']\n",
    "\n",
    "        # %%\n",
    "        temp_lengths = temp_series.str.split(', ').str.len().astype(int) # Number of loans\n",
    "\n",
    "        # %%\n",
    "        temp_lengths_max = temp_lengths.max()\n",
    "\n",
    "        # %%\n",
    "        for index, val in temp_lengths.items():\n",
    "            temp_series[index] = (temp_lengths_max - val) * 'No Loan, ' + temp_series[index]\n",
    "\n",
    "        # %%\n",
    "        temp_series.head()\n",
    "\n",
    "        # %%\n",
    "        temp = temp_series.str.split(pat = ', ', expand = True)\n",
    "        unique_loans = set()\n",
    "        for col in temp.columns:\n",
    "            temp[col] = temp[col].str.lstrip('and ')\n",
    "            unique_loans.update(temp[col].unique())\n",
    "        # print(unique_loans)\n",
    "\n",
    "        # %%\n",
    "        len(unique_loans)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # There are total 8 unique type of loans, one placeholder for no specification of loan type and one placeholder added by us to specify there is no loan.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # When we are working with tree based models usually they don't need the categorical columns to be encoded to numerical data type like we need for linear regression, logistic regression etc. as the model can handle these. But scikit learn uses CART algorithm and there is no functionality of using categorical variables directly and thus, for modelling with scikit-learn they need to be numerically encoded. Right now in this column we have 6260 unique categories which is too much to search for at one node. as at each node, a decision tree will look at all possible values of the categorical column to figure out what value produces the best split based on gini impurity or entropy decrease on splitting. We can do some pre-prcoessing on this column to split it into multiple columns. Intutively, it feels like the lastest loan should have high influence on your credit score because if the loan is heavy in nature then it might lead to poor credit score if unable to pay while if it is light then the credit score should remain almost same as before. We can split the column in following format: Latest loan1, latest loan2 etc. This way we will have 9 columns corresponding to maximum number of loans for any customer and each column can have maximum 10 categorical values corresponding to the unqiue loans calculated above. \n",
    "\n",
    "        # %% [markdown]\n",
    "        # This approach will have following benefits:  \n",
    "        # 1. Preserves the order of loans even after splitting.\n",
    "        # 2. Easier to visualize and understand patterns since number of categories per column reduces.    \n",
    "        # 3. Algorithm can focus more on information contained within the loan sequence. For example, if second last and third last loan contain critical information in classifying credit score than focussing on whole sequence of loans is not worthwhile and this inturn might lead to smaller decision trees and faster training compared to if we didn't split.  \n",
    "        # 4. At each node for 9 of the splitted columns only total 90(9 columns * 10 categories) comparisons need to be made after one-hot encoding rather than 6260 comparisons for one non-splitted column.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Another possible approach could be to split columns as first loan, second loan etc. as a way of splitting but the pre-processing mentioned above feels more effective for now. We will maybe look at these two modeling startegies whn we do EDA and modeling.\n",
    "\n",
    "        # %%\n",
    "        temp.columns = [f'Last_Loan_{i}' for i in range(int(df['Num_of_Loan'].max()), 0, -1)]\n",
    "\n",
    "        # %%\n",
    "        temp.head()\n",
    "\n",
    "        # %%\n",
    "        df = pd.merge(df, temp, left_index = True, right_index = True)\n",
    "\n",
    "        # %%\n",
    "        df.head()\n",
    "        df.drop(columns = 'Type_of_Loan', inplace = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 9. Delay from due date\n",
    "\n",
    "        # %%\n",
    "        summary_due_date = summarize_numerical_column_with_deviation(df, 'Delay_from_due_date', median_standardization_summary = True)\n",
    "\n",
    "        # %%\n",
    "        summary_due_date\n",
    "\n",
    "        # %% [markdown]\n",
    "        # The median standardization varies quite a bit going from -10 to 11. \n",
    "\n",
    "        # %%\n",
    "        due_date_deviation = df.groupby('Customer_ID')['Delay_from_due_date'].transform(median_standardization, default_value = return_max_MAD(df, 'Delay_from_due_date'))\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Looking at the fact that overall distribution of delay from due date is not too extreme and delay from due date can vary a lot as well unlike number of credit cards or number of bank accounts. We will move forward with the data as it is. Having a domain expert by your side would have helped make this more clearer.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 10. Number of delayed payments\n",
    "\n",
    "        # %%\n",
    "        summary_num_delayed_payments = summarize_numerical_column_with_deviation(df, 'Num_of_Delayed_Payment', median_standardization_summary = True)\n",
    "\n",
    "        # %%\n",
    "        summary_num_delayed_payments\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Judging from median standardization, almost all of the values are same as median and this is leading to 0 median standardization. Median standardization should definitely should be like this and should be skewed in nature but its hard to assess what threshold to use without a domain expert. We will use the full column as a sample and judge based on that here.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # The number of delayed payments can't be too much and can not be negative as well. We will set negative values and values greater than upper range of oultiers to null.\n",
    "\n",
    "        # %%\n",
    "        df['Num_of_Delayed_Payment'][(df['Num_of_Delayed_Payment'] > summary_num_delayed_payments['Num_of_Delayed_Payment']['Outlier upper range']) | (df['Num_of_Delayed_Payment'] < 0)] = np.nan\n",
    "\n",
    "        # %%\n",
    "        df['Num_of_Delayed_Payment'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # There are 8382 null values. Lets observe the count of diff in between consecutive months and observe if we can identify some pattern exising there which can help us make some educated guess about the null values.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(pd.Series.diff).value_counts(normalize = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Around 45.6% of time it remains same across months but rest of the time it varies i.e more than 50% of the time it varies across months.\n",
    "\n",
    "        # %%\n",
    "        df[['Customer_ID', 'Num_of_Delayed_Payment']].head(40)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Looking at the data it looks like usually a single value repeats more often across months i.e. mode might be a suitable choice here. But first lets see that usually how many times the mode occurs for any customer.\n",
    "\n",
    "        # %%\n",
    "        # Ratio of frequency of mode and number of non-null data per customer\n",
    "        temp = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(lambda x: (x == x.mode()[0]).sum()/x.notnull().sum()).value_counts(normalize = True)\n",
    "\n",
    "        # %%\n",
    "        temp[temp.index > 0.5].sum() # Idenitfying how many times the mode occurs in more than 50% of non-null data per customer\n",
    "\n",
    "        # %% [markdown]\n",
    "        # That is within given data for around 75.8% of the customers the mode occurs more than 50% of the time within 8-months period for whatever data we have available. This means the mode might be a suitable imputation here.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # What if there are multiple modes per customer? Lets check the data if such thing exists.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Num_of_Delayed_Payment'].agg(lambda x: len(x.mode())).value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Mostly, we observe one mode but sometimes it can be more than one as well. What to do in multiple modes case? We can take some average or median of modes in that case, in case there is skewness within the modes, median would be a better guess and in case where medians come out to be floating point number we can just take the integer part as an approximation.\n",
    "\n",
    "        # %%\n",
    "        df['Num_of_Delayed_Payment'] = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(return_mode_median_filled_int).astype(int)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Number of delayed payments is something that can vary from month to month its not like it has monotically increasing pattern or so which we can use as a sanity check. What we can try to observe is the relative devaition from median for this cleaned column.\n",
    "\n",
    "        # %%\n",
    "        summarize_numerical_column_with_deviation(df, 'Num_of_Delayed_Payment', median_standardization_summary = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 11. Changed credit limit\n",
    "\n",
    "        # %%\n",
    "        summary_changed_credit_limit = summarize_numerical_column_with_deviation(df, 'Changed_Credit_Limit', median_standardization_summary = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Credit card limit is dependent upon the users usage patterns. If the lender trusts the customer then it can increase also and if customer is late on payments, low activity etc. then the credit lmit can decrease as well. Thus, both negative and positive values are understandable.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # The upper range for outliers for full column doesn't significantly deviate from the max value and its difficult to judge here what threshold should be placed on credit limit median standardization. Thus, we leave non-null values as it is for now.\n",
    "\n",
    "        # %%\n",
    "        df[['Customer_ID', 'Changed_Credit_Limit']].head(40)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Looking at the data usually the credit limit occurs with the same value across months i.e. the mode might be an appropriate value to imputate. Lets do some checks first though.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Changed_Credit_Limit'].agg(lambda x: len(x.mode())).value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Almost all the time only one mode appears. But sometimes two mode can occur as well, since this is a floating point type feature and there are only two mode values we will choose average of both which will be same as median in this case.\n",
    "\n",
    "        # %%\n",
    "        df['Changed_Credit_Limit'] = df.groupby('Customer_ID')['Changed_Credit_Limit'].transform(return_mode_average_filled)\n",
    "\n",
    "        # %%\n",
    "        df['Changed_Credit_Limit'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 12. Number of credit card inquiries\n",
    "\n",
    "        # %%\n",
    "        summary_num_credit_inquiries = summarize_numerical_column_with_deviation(df, 'Num_Credit_Inquiries', median_standardization_summary = True)\n",
    "        df['Num_Credit_Inquiries'][(df['Num_Credit_Inquiries'] > summary_num_credit_inquiries['Num_Credit_Inquiries']['Outlier upper range']) | (df['Num_Credit_Inquiries'] < 0)] = np.nan\n",
    "\n",
    "        # %%\n",
    "        df['Num_Credit_Inquiries'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Lets look at some data.\n",
    "\n",
    "        # %%\n",
    "        df[['Customer_ID', 'Num_Credit_Inquiries']].head(40)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # This a type of data which is monotically increasing in nature and thus should increase or remain same as months go on. Lets check that.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # As expected it mostly either remains same or increases. In this case we can just use forward fill and backward fill to fill these nulls.\n",
    "\n",
    "        # %%\n",
    "        df['Num_Credit_Inquiries'] = df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(forward_backward_fill).astype(int)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Lets do the check again.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 13. Credit Mix\n",
    "        df['Credit_Mix'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # A lot of null values present. We have already seen that during the 8-months period the type of loans and number of loans remain same so its fair to assume credit mix will also remain same. Lets check that with the given data.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Credit_Mix'].nunique().value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # We can observe that one customer has only one type of credit mix only, throughout the 8-months period apart from null values. We can just use forward fill and bacward fill to achieve the desired goal.\n",
    "\n",
    "        # %%\n",
    "        df['Credit_Mix'] = df.groupby('Customer_ID')['Credit_Mix'].transform(forward_backward_fill)\n",
    "\n",
    "        # %%\n",
    "        df['Credit_Mix'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 14. Outstanding debt\n",
    "\n",
    "        # %%\n",
    "        summary_outstanding_debt = summarize_numerical_column_with_deviation(df, 'Outstanding_Debt', median_standardization_summary = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # All the values in median standardization are coming out to be zero. Is the outstanding debt constant for each customer across months after ignoring nulls?\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Outstanding_Debt'].nunique().value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # The column looks ok from the distribution perspective and there are no nulls present.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 15. Credit Utilization ratio\n",
    "\n",
    "        # %%\n",
    "        summary_credit_utilization_ratio = summarize_numerical_column_with_deviation(df, 'Credit_Utilization_Ratio', median_standardization_summary = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Judging from both the graphs its hard to put a threshold on median standardization of credit utilization ratio without a domin expert and also, the distribution of the column as a whole looks decent enough to not touch it further.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 16. Credit History Age\n",
    "\n",
    "        # %%\n",
    "        df[['Customer_ID', 'Credit_History_Age']].head(40)\n",
    "\n",
    "        # %%\n",
    "        df['Credit_History_Age'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # There are 9030 null values. Looking at the data it is of the format - '{Year} Years and {Months} Months'. Using str functions of pandas series, we can extract these two data values i.e. year and months. We will then combine them in a single column as total months because both the year data and month data can be easily extracted from total months so there will be no loss of information and we will be able to reduce one feature from our dataset.\n",
    "\n",
    "        # %%\n",
    "        df[['Years', 'Months']] = df['Credit_History_Age'].str.extract('(?P<Years>\\d+) Years and (?P<Months>\\d+) Months').astype(float)\n",
    "        df[['Years', 'Months']].describe()\n",
    "        df['Credit_History_Age'] = df['Years'] * 12 + df['Months']\n",
    "\n",
    "        # %%\n",
    "        df.drop(columns = ['Years', 'Months'], inplace = True)\n",
    "\n",
    "        df['Credit_History_Age'].isnull().sum()\n",
    "        df['Credit_History_Age'] = df.groupby('Customer_ID')['Credit_History_Age'].transform(fill_month_history).astype(int)\n",
    "\n",
    "        # #### 17. Payment of minimum amount\n",
    "        df['Payment_of_Min_Amount'].value_counts()\n",
    "\n",
    "        df.groupby(['Customer_ID'])['Payment_of_Min_Amount'].nunique().value_counts()\n",
    "\n",
    "        df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map({'Yes': 1, 'No': 0, 'NM': np.nan})\n",
    "\n",
    "        df.groupby('Customer_ID')['Payment_of_Min_Amount'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "        df['Payment_of_Min_Amount'] = df.groupby('Customer_ID')['Payment_of_Min_Amount'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "        df['Payment_of_Min_Amount'].isnull().sum()\n",
    "        df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map({1: 'Yes', 0: 'No'})\n",
    "\n",
    "        # #### 18. Total EMI per month\n",
    "        summary_total_emi_per_month = summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)\n",
    "        summary_total_emi_per_month\n",
    "\n",
    "        deviation_total_emi = df.groupby('Customer_ID')['Total_EMI_per_month'].transform(median_standardization, default_value = return_max_MAD(df, 'Total_EMI_per_month'))\n",
    "\n",
    "        df['Total_EMI_per_month'][deviation_total_emi > 10000] = np.nan\n",
    "        summary_total_emi_per_month = summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)\n",
    "        df['Total_EMI_per_month'][(df['Total_EMI_per_month'] > summary_total_emi_per_month['Total_EMI_per_month']['Outlier upper range'])] = np.nan\n",
    "        df['Total_EMI_per_month'].isnull().sum()\n",
    "        df.groupby('Customer_ID')['Total_EMI_per_month'].nunique().value_counts()\n",
    "        deviation_total_emi = df_copy.groupby('Customer_ID', group_keys = False)['Total_EMI_per_month'].transform(median_standardization, default_value = return_max_MAD(df_copy, 'Total_EMI_per_month'))\n",
    "\n",
    "        # %%\n",
    "        temp = (deviation_total_emi[df.groupby('Customer_ID')['Total_EMI_per_month'].transform(pd.Series.nunique) == 0])\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Looking at this, only one value looks absurdly big. The rest of the median standardization's could even be considered ok for now.\n",
    "\n",
    "        # %%\n",
    "        temp[temp > 80]\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Lets convert this value to null and feed it back to the dataset.\n",
    "\n",
    "        # %%\n",
    "        temp[79370] = np.nan\n",
    "        df['Total_EMI_per_month'][temp.index] = temp\n",
    "\n",
    "        # %%\n",
    "        summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Now the data looks more appropriate compared to before. There are still 4420 null values which need to be handled here.\n",
    "\n",
    "        # %% [markdown]\n",
    "        # The rest of the null values can be filled using forward and backward fill as the EMI's should be highly dependent upon previous month.\n",
    "\n",
    "        # %%\n",
    "        df['Total_EMI_per_month'] = df.groupby('Customer_ID')['Total_EMI_per_month'].transform(forward_backward_fill)\n",
    "\n",
    "        # %%\n",
    "        df['Total_EMI_per_month'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 19. Amount Invested Monthly\n",
    "\n",
    "        # %%\n",
    "        summary_amount_invested_monthly = summarize_numerical_column_with_deviation(df, 'Amount_invested_monthly', median_standardization_summary = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Some values in amount invested monthly are too extreme compared to the rest of the data and thus, can be removed considering them to be erroneous before we do further processing.\n",
    "\n",
    "        # %%\n",
    "        df['Amount_invested_monthly'][df['Amount_invested_monthly'] > 8000] = np.nan\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Lets check the distribution again.\n",
    "\n",
    "        # %%\n",
    "        summary_amount_invested_monthly = summarize_numerical_column_with_deviation(df, 'Amount_invested_monthly', median_standardization_summary = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Looks like power law distribution, hopefully these are not erroneous values. Lets leave these non-null values as it is for now. Null values still need to be handled.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Amount_invested_monthly'].transform(return_num_of_modes).value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Lets choose the median of values as a decent approximation for null values.\n",
    "\n",
    "        # %%\n",
    "        df['Amount_invested_monthly'] = df.groupby('Customer_ID')['Amount_invested_monthly'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 20. Payment Behaviour\n",
    "        df['Payment_Behaviour'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # 7600 of null values present.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Payment_Behaviour'].nunique().value_counts()\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Payment_Behaviour'].agg(return_num_of_modes).value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # The number of modes vary, if the number of mode is 1 then we can use that for imputation else forward fill and backward fill can be used.\n",
    "\n",
    "        # %%\n",
    "        df['Payment_Behaviour'] = df.groupby('Customer_ID')['Payment_Behaviour'].transform(lambda x: return_mode(x) if len(x.mode()) == 1 else forward_backward_fill(x))\n",
    "\n",
    "        # %%\n",
    "        df['Payment_Behaviour'].isnull().sum()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # #### 21. Monthly Balance\n",
    "\n",
    "        # %%\n",
    "        summary_monthly_balance = summarize_numerical_column_with_deviation(df, 'Monthly_Balance', median_standardization_summary = True)\n",
    "\n",
    "        # %% [markdown]\n",
    "        # There are 1209 null values. Looking at the column as a whole the distribution looks ok and considering the fact that we can't decide exactly on a threshold on median standardization without domain expertise. We will leave the non-null values as it for now.\n",
    "\n",
    "        # %%\n",
    "        df.groupby('Customer_ID')['Monthly_Balance'].nunique().value_counts()\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Since there might be skewness within the data we can use median to fill the null values.\n",
    "\n",
    "        # %%\n",
    "        df['Monthly_Balance'] = df.groupby('Customer_ID')['Monthly_Balance'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "        # %% [markdown]\n",
    "        # ### Deleting unnecessary columns\n",
    "\n",
    "        # %%\n",
    "        df.columns\n",
    "\n",
    "        # %% [markdown]\n",
    "        # Month column is not needed anymore and can be dropped. We will keep customer id as it is for now so that it can be used later on when doing train-test splits.\n",
    "\n",
    "        # %%\n",
    "        df.drop(columns = ['Month'], inplace = True)\n",
    "\n",
    "        # %%\n",
    "        df = df.sample(frac = 1) #shuffle data\n",
    "\n",
    "        # %% [markdown]\n",
    "        # ### Rearranging the columns\n",
    "\n",
    "        # %%\n",
    "        df.columns\n",
    "\n",
    "        # %%\n",
    "        df = df.loc[:, ['Customer_ID', 'Age', 'Occupation', 'Annual_Income',\n",
    "            'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
    "            'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
    "            'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n",
    "            'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt',\n",
    "            'Credit_Utilization_Ratio', 'Credit_History_Age',\n",
    "            'Payment_of_Min_Amount', 'Total_EMI_per_month',\n",
    "            'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance', 'Last_Loan_9', 'Last_Loan_8', 'Last_Loan_7',\n",
    "            'Last_Loan_6', 'Last_Loan_5', 'Last_Loan_4', 'Last_Loan_3',\n",
    "            'Last_Loan_2', 'Last_Loan_1',\n",
    "            'Credit_Score']]\n",
    "\n",
    "        # Total rows after cleaning\n",
    "        total_rows_after_cleaning = len(df)\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Total Rows after cleaning is: {total_rows_after_cleaning}\")\n",
    "        # Export the cleaned data\n",
    "        parent_dir = os.path.dirname(os.getcwd())\n",
    "        file__out_path = os.path.join(parent_dir, \"data\\Credit_score_cleaned_data.csv\")\n",
    "        df.to_csv(file__out_path, index = False)\n",
    "        data_cleaned = True\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        print(f\"\\nTime to process is: {processing_time:.2f} seconds\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data cleaning: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Input For Neural Networks\n",
    "Drop or add columns that for the model.\n",
    "This is where you control your input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN():\n",
    "    global df, X_train, X_test, y_train, y_test, model, encoder, data_loaded, data_cleaned, model_trained\n",
    "    try:\n",
    "        if not data_loaded:  # Check if data is loaded\n",
    "            print(\"Error: Data has not been loaded. Please load the data first using option (1).\")\n",
    "            return\n",
    "        if not data_cleaned:\n",
    "            print(\"Error: Data has not been cleaned. Please clean the data first using option (2).\")\n",
    "            return\n",
    "\n",
    "        print(\"Train NN:\")\n",
    "        print(\"********\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # target and features\n",
    "        target = ['Credit_Score']\n",
    "        continuous_features = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Credit_Card', 'Interest_Rate', 'Credit_Utilization_Ratio', 'Credit_History_Age','Monthly_Balance', 'Num_Credit_Inquiries', 'Outstanding_Debt', 'Num_of_Delayed_Payment', 'Delay_from_due_date', 'Changed_Credit_Limit', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Num_Bank_Accounts', 'Num_of_Loan'] \n",
    "        categorical_features = ['Credit_Mix', 'Payment_Behaviour','Last_Loan_5', 'Last_Loan_4', 'Last_Loan_3','Last_Loan_2', 'Last_Loan_1', 'Payment_of_Min_Amount', 'Payment_of_Min_Amount', 'Last_Loan_9','Last_Loan_8', 'Last_Loan_7', 'Last_Loan_6', 'Occupation']\n",
    "\n",
    "        # Encoding features and target\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        # Encoding categorical features\n",
    "        encoded_features = encoder.fit_transform(df[categorical_features])\n",
    "\n",
    "        # Convert the encoded data back to a DataFrame:\n",
    "        encoded_df = pd.DataFrame(encoded_features.toarray(), columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "        # joining dataframes \n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "        # Encoding categorical features\n",
    "        encoded_target = encoder.fit_transform(df[target])\n",
    "\n",
    "        # Convert the encoded data back to a DataFrame:\n",
    "        encoded_target_df = pd.DataFrame(encoded_target.toarray(), columns=encoder.get_feature_names_out(target))\n",
    "\n",
    "        # joining dataframes \n",
    "        df = pd.concat([df, encoded_target_df], axis=1)\n",
    "        # Constructing dataframe for modeling\n",
    "        features_for_model = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Credit_Card', 'Interest_Rate'\n",
    "                            , 'Credit_Utilization_Ratio', 'Credit_Mix_Bad', 'Credit_Mix_Good', 'Credit_Mix_Standard'\n",
    "                            , 'Last_Loan_3','Last_Loan_2', 'Last_Loan_1', 'Credit_History_Age','Monthly_Balance'\n",
    "                            , 'Payment_Behaviour', 'Num_Credit_Inquiries','Last_Loan_5', 'Last_Loan_4', 'Outstanding_Debt'\n",
    "                            , 'Num_of_Delayed_Payment', 'Delay_from_due_date', 'Payment_of_Min_Amount', 'Changed_Credit_Limit'\n",
    "                            , 'Total_EMI_per_month', 'Amount_invested_monthly', 'Num_Bank_Accounts', 'Num_of_Loan', 'Last_Loan_9'\n",
    "                            ,'Last_Loan_8', 'Last_Loan_7', 'Last_Loan_6', 'Occupation_Accountant', 'Occupation_Architect'\n",
    "                            , 'Occupation_Developer', 'Occupation_Doctor', 'Occupation_Engineer', 'Occupation_Entrepreneur'\n",
    "                            , 'Occupation_Journalist', 'Occupation_Lawyer', 'Occupation_Manager', 'Occupation_Mechanic'\n",
    "                            ,'Occupation_Media_Manager' , 'Occupation_Musician', 'Occupation_Scientist', 'Occupation_Teacher'\n",
    "                            , 'Occupation_Writer'\n",
    "                            ] \n",
    "\n",
    "        target_features = ['Credit_Score_Good', 'Credit_Score_Poor', 'Credit_Score_Standard']\n",
    "\n",
    "        # Defining data sets\n",
    "        X = encoded_features.toarray()\n",
    "        y = encoded_target.toarray()\n",
    "\n",
    "        # Basic train-test split\n",
    "        # 80% training and 20% test \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.20, random_state=42)\n",
    "\n",
    "        # Create network topology\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # Adding input model --> 24 input layers\n",
    "        model.add(Dense(46, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "\n",
    "        # Adding hidden layer \n",
    "        model.add(keras.layers.Dense(1000, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(512, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "        # # output layer\n",
    "        # # For classification tasks, we generally tend to add an activation function in the output (\"sigmoid\" for binary, and \"softmax\" for multi-class, etc.).\n",
    "        model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "        # print(model.summary())\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X_train, y_train, epochs = 30, batch_size = 120)\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = y_pred.argmax(axis=1)\n",
    "        y_test_decoded = y_test.argmax(axis=1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = test_acc\n",
    "        precision = precision_score(y_test_decoded, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test_decoded, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test_decoded, y_pred, average='weighted')\n",
    "        confusion_matrix_result = confusion_matrix(y_test_decoded, y_pred)\n",
    "\n",
    "        # Logging metrics\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Model Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Model Precision: {precision:.4f}\")\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Model Recall: {recall:.4f}\")\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Model f1_score: {f1:.4f}\")\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Model Confusion Matrix: \\n{confusion_matrix_result}\")\n",
    "        model_trained = True\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nTime to process: {end_time - start_time:.2f} seconds\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions():\n",
    "    global X_train, X_test, y_test, model, encoder, data_loaded, data_cleaned, model_trained\n",
    "    try:\n",
    "        # Check if the data was loaded\n",
    "        if not data_loaded:\n",
    "            raise ValueError(\"Data has not been loaded. Please load the data first using option (1).\")\n",
    "\n",
    "        # Check if the data was cleaned\n",
    "        if not data_cleaned:\n",
    "            raise ValueError(\"Data has not been cleaned. Please clean the data first using option (2).\")\n",
    "\n",
    "        # Check if the model was trained\n",
    "        if not model_trained:\n",
    "            raise ValueError(\"The model has not been trained. Please train the model first using option (3).\")\n",
    "\n",
    "        print(\"Generate Predictions:\")\n",
    "        print(\"********************\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generate predictions\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Generating prediction using selected Neural Network\")\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Logging the size of datasets\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Size of training set: {X_train.shape[0]}\")\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Size of testing set: {X_test.shape[0]}\")\n",
    "\n",
    "        # Decode predictions and true labels\n",
    "        y_tested = y_test.argmax(axis=1)  # For multi-class classification\n",
    "        y_predicted = predictions.argmax(axis=1)\n",
    "\n",
    "        # Map numbers back to labels\n",
    "        label_mapping = {0: 'Poor', 1: 'Standard', 2: 'Good'}\n",
    "        y_tested_labels = [label_mapping[label] for label in y_tested]\n",
    "        y_predicted_labels = [label_mapping[label] for label in y_predicted]\n",
    "\n",
    "        # Save predictions to CSV\n",
    "        predictions_df = pd.DataFrame({\n",
    "            \"True Value\": y_tested_labels,\n",
    "            \"Predicted Value\": y_predicted_labels\n",
    "        })\n",
    "        parent_dir = os.path.dirname(os.getcwd())\n",
    "        file_out_path = os.path.join(parent_dir, \"data\\predictions.csv\")\n",
    "        predictions_df.to_csv(file_out_path, index=False)\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Predictions generated (predictions.csv have been generated)....\")\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        class_labels = ['Good', 'Poor', 'Standard']\n",
    "        # plot_prediction_vs_test_categorical(y_tested_labels, y_predicted_labels, class_labels)\n",
    "\n",
    "        # Calculate model performance\n",
    "        calculate_performance_multiclass(y_tested_labels, y_predicted_labels)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nTime to process: {end_time - start_time:.2f} seconds\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during predictions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "Menu:\n",
      "(1) Load data\n",
      "(2) Process (Clean) data\n",
      "(3) Train NN\n",
      "(4) Generate Predictions\n",
      "(5) Quit\n",
      "===============================================\n",
      "\n",
      "Loading and cleaning input data set:\n",
      "************************************\n",
      "[2024-12-05 21:39:09] Starting Script\n",
      "[2024-12-05 21:39:09] Loading training data set\n",
      "[2024-12-05 21:39:09] Total Columns Read: 29\n",
      "[2024-12-05 21:39:09] Total Rows Read: 80000\n",
      "\n",
      "Time to load is: 0.24 seconds\n",
      "\n",
      "Returning to the main menu. Please try again.\n",
      "\n",
      "===============================================\n",
      "Menu:\n",
      "(1) Load data\n",
      "(2) Process (Clean) data\n",
      "(3) Train NN\n",
      "(4) Generate Predictions\n",
      "(5) Quit\n",
      "===============================================\n",
      "Process (Clean) data:\n",
      "*********************\n",
      "2024-12-05 21:39:14 Performing Data Clean Up\n",
      "2024-12-05 21:41:02 Total Rows after cleaning is: 80000\n",
      "\n",
      "Time to process is: 108.39 seconds\n",
      "\n",
      "===============================================\n",
      "Menu:\n",
      "(1) Load data\n",
      "(2) Process (Clean) data\n",
      "(3) Train NN\n",
      "(4) Generate Predictions\n",
      "(5) Quit\n",
      "===============================================\n",
      "Train NN:\n",
      "********\n",
      "Epoch 1/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6005 - loss: 0.8224\n",
      "Epoch 2/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6386 - loss: 0.7683\n",
      "Epoch 3/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6682 - loss: 0.7264\n",
      "Epoch 4/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6937 - loss: 0.6795\n",
      "Epoch 5/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7144 - loss: 0.6396\n",
      "Epoch 6/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7383 - loss: 0.5898\n",
      "Epoch 7/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7482 - loss: 0.5603\n",
      "Epoch 8/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7636 - loss: 0.5286\n",
      "Epoch 9/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7717 - loss: 0.5053\n",
      "Epoch 10/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7857 - loss: 0.4815\n",
      "Epoch 11/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7865 - loss: 0.4671\n",
      "Epoch 12/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7932 - loss: 0.4510\n",
      "Epoch 13/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7968 - loss: 0.4410\n",
      "Epoch 14/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8011 - loss: 0.4275\n",
      "Epoch 15/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8037 - loss: 0.4196\n",
      "Epoch 16/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8043 - loss: 0.4217\n",
      "Epoch 17/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8032 - loss: 0.4139\n",
      "Epoch 18/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8103 - loss: 0.4093\n",
      "Epoch 19/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8086 - loss: 0.4018\n",
      "Epoch 20/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8099 - loss: 0.4008\n",
      "Epoch 21/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8136 - loss: 0.3955\n",
      "Epoch 22/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8165 - loss: 0.3908\n",
      "Epoch 23/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8153 - loss: 0.3887\n",
      "Epoch 24/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8163 - loss: 0.3874\n",
      "Epoch 25/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8151 - loss: 0.3878\n",
      "Epoch 26/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8150 - loss: 0.3875\n",
      "Epoch 27/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8184 - loss: 0.3816\n",
      "Epoch 28/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8215 - loss: 0.3773\n",
      "Epoch 29/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8203 - loss: 0.3784\n",
      "Epoch 30/30\n",
      "\u001b[1m534/534\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8164 - loss: 0.3853\n",
      "500/500 - 0s - 748us/step - accuracy: 0.7749 - loss: 0.6472\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step\n",
      "2024-12-05 21:43:15 Model Accuracy: 0.7749\n",
      "2024-12-05 21:43:15 Model Precision: 0.7806\n",
      "2024-12-05 21:43:15 Model Recall: 0.7749\n",
      "2024-12-05 21:43:15 Model f1_score: 0.7760\n",
      "2024-12-05 21:43:15 Model Confusion Matrix: \n",
      "[[2152   47  652]\n",
      " [ 195 3826  654]\n",
      " [ 896 1157 6421]]\n",
      "\n",
      "Time to process: 64.52 seconds\n",
      "\n",
      "===============================================\n",
      "Menu:\n",
      "(1) Load data\n",
      "(2) Process (Clean) data\n",
      "(3) Train NN\n",
      "(4) Generate Predictions\n",
      "(5) Quit\n",
      "===============================================\n",
      "Generate Predictions:\n",
      "********************\n",
      "2024-12-05 21:44:48 Generating prediction using selected Neural Network\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step\n",
      "2024-12-05 21:44:49 Size of training set: 64000\n",
      "2024-12-05 21:44:49 Size of testing set: 16000\n",
      "2024-12-05 21:44:49 Predictions generated (predictions.csv have been generated)....\n",
      "\n",
      "Time to process: 0.55 seconds\n",
      "\n",
      "===============================================\n",
      "Menu:\n",
      "(1) Load data\n",
      "(2) Process (Clean) data\n",
      "(3) Train NN\n",
      "(4) Generate Predictions\n",
      "(5) Quit\n",
      "===============================================\n",
      "Exiting the program.\n"
     ]
    }
   ],
   "source": [
    "def mainMenu():\n",
    "    global df, X_test, y_test, model, encoder\n",
    "    while True:\n",
    "        print(\"\\n===============================================\")\n",
    "        print(\"Menu:\")\n",
    "        print(\"(1) Load data\")\n",
    "        print(\"(2) Process (Clean) data\")\n",
    "        print(\"(3) Train NN\")\n",
    "        print(\"(4) Generate Predictions\")\n",
    "        print(\"(5) Quit\")\n",
    "        print(\"===============================================\")\n",
    "        \n",
    "        choice = input(\"Select Option: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            loadData()\n",
    "        \n",
    "        elif choice == '2':\n",
    "            dataCleaning()\n",
    "                \n",
    "        elif choice == '3':\n",
    "            trainNN()\n",
    "        \n",
    "        elif choice == '4':\n",
    "            predictions()\n",
    "        \n",
    "        elif choice == '5':\n",
    "            print(\"Exiting the program.\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid option. Please select a valid option.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainMenu()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
