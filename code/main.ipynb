{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Necessities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Packages\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# data handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# visualization libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# extra libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Packages to support NN\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a folder called data within my project file\n",
    "#  .. my_project\n",
    "#     |\n",
    "#     |___code\n",
    "#     |   |\n",
    "#     |   |__ CS3500_Starter_Notebook.ipynb\n",
    "#     |\n",
    "#     |___data\n",
    "#         |\n",
    "#         |__ credit_score.csv\n",
    "#\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# # Get the current working directory\n",
    "# current_dir = os.getcwd() \n",
    "\n",
    "# # Construct a path to the parent directory\n",
    "# parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# # Access a file in the parent directory\n",
    "# file_path = os.path.join(parent_dir, \"data/credit_score_data.csv\")\n",
    "\n",
    "# # Load Credit Score data\n",
    "# df = pd.read_csv(file_path) \n",
    "\n",
    "def loadData():\n",
    "    global df\n",
    "    \"\"\"Load the credit score dataset and display summary statistics.\"\"\"\n",
    "    print(\"\\nLoading and cleaning input data set:\")\n",
    "    print(\"************************************\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd() \n",
    "\n",
    "    # Construct a path to the parent directory\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "    # Access a file in the parent directory\n",
    "    file_path = os.path.join(parent_dir, \"data\\credit_score_data.csv\")\n",
    "    \n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting Script\")\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Loading training data set\")\n",
    "    \n",
    "    try:\n",
    "        # Load Credit Score data\n",
    "        df = pd.read_csv(file_path) \n",
    "        total_columns = df.shape[1]\n",
    "        total_rows = df.shape[0]\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Total Columns Read: {total_columns}\")\n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Total Rows Read: {total_rows}\")\n",
    "        print(f\"\\nTime to load is: {round(end_time - start_time, 2)} seconds\")\n",
    "        \n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Error: File not found at {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions For Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_numerical_column(series, col_name):\n",
    "    '''Describe a numerical column using describe function of series, report number of null values, display boxplots and histograms.\n",
    "    Return min, max, IQR based outlier lower range and IQR based outlier upper range as a dictionary.'''\n",
    "    # print(series.describe(), end = '\\n\\n')\n",
    "\n",
    "    # print(f'Number of null values: {series.isnull().sum()}', '\\n\\n')\n",
    "    \n",
    "    # fig, ax = plt.subplots(2, 1, figsize = (8, 8), sharex = True)\n",
    "    # sns.boxplot(series, orient = 'h', ax = ax[0])\n",
    "    # ax[0].set_title(f'Distribution of {col_name}')\n",
    "    # ax[0].tick_params(left = False, labelleft = False) \n",
    "    # sns.histplot(series, ax = ax[1])\n",
    "    # ax[1].set_ylabel('Frequency')\n",
    "    # ax[1].set_xlabel(col_name)\n",
    "    # plt.show();\n",
    "\n",
    "    q1, q3 = series.quantile([0.25, 0.75])\n",
    "    IQR = q3 - q1\n",
    "    \n",
    "    return  {'Min. value': series.min(), 'Outlier lower range': q1 - 1.5 * IQR, 'Outlier upper range': q3 + 1.5 * IQR, 'Max. value': series.max()}\n",
    "\n",
    "def summarize_numerical_column_with_deviation(data, num_col, group_col = 'Customer_ID', absolute_summary = True, median_standardization_summary = False):\n",
    "    '''Summarize the numerical column and its median standardization based on customers using describe_numerical_column function.'''\n",
    "    Summary_dict = {}\n",
    "    \n",
    "    if absolute_summary == True:\n",
    "        # print(f'Column description for {num_col}:\\n')\n",
    "        Summary_dict[num_col] = describe_numerical_column(data[num_col], num_col)\n",
    "        \n",
    "    if median_standardization_summary == True:\n",
    "        # if absolute_summary == True:\n",
    "        #     print('\\n')\n",
    "        default_MAD = return_max_MAD(data, num_col, group_col)\n",
    "        num_col_standardization = data.groupby(group_col)[num_col].apply(median_standardization, default_value = default_MAD)\n",
    "        # print(f'Median standardization for {num_col}:\\n')\n",
    "        Summary_dict[f'Median standardization of {num_col}'] = describe_numerical_column(num_col_standardization, f'Median standardization of {num_col}')\n",
    "        Summary_dict['Max. MAD'] = default_MAD\n",
    "    return Summary_dict\n",
    "\n",
    "def return_max_MAD(data, num_col, group_col = 'Customer_ID'):\n",
    "    '''Return max value of median absolute devaition(MAD) from within the customers for num_col'''\n",
    "    return (data.groupby(group_col)[num_col].agg(lambda x: (x - x.median()).abs().median())).max()\n",
    "    \n",
    "def validate_age(x):\n",
    "    '''Check whether 8-months period age for a customer is logically valid or not'''\n",
    "    diff = x.diff()\n",
    "    if (diff == 0).sum() == 7:\n",
    "        return True\n",
    "    elif ((diff.isin([0, 1])).sum() == 7) and ((diff == 1).sum() == 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "def median_standardization(x, default_value):\n",
    "    '''Transform series or dataframe to its devaition from median with respect to Median absolute deviation(MAD) i.e. median standardization.'''\n",
    "    med = x.median() \n",
    "    abs = (x - med).abs()\n",
    "    MAD = abs.median()\n",
    "    if MAD == 0:\n",
    "        if ((abs == 0).sum() == abs.notnull().sum()): # When MAD is zero and all non-null values are constant in x\n",
    "            return x * 0\n",
    "        else:\n",
    "            return (x - med)/default_value # When MAD is zero but all non-values are not same in x\n",
    "    else:\n",
    "        return (x - med)/MAD # When MAD is non-zero\n",
    "\n",
    "def return_num_of_modes(x):\n",
    "    '''Return number of modes in given series or dataframe'''\n",
    "    return len(x.mode())\n",
    "\n",
    "def return_mode(x):\n",
    "    '''Return nan if no mode exists in given series or return minimum mode'''\n",
    "    modes = x.mode()\n",
    "    if len(modes) == 0:\n",
    "        return np.nan\n",
    "    return modes.min()\n",
    "\n",
    "def forward_backward_fill(x):\n",
    "    '''Perform forward fill then backward fill on given series or dataframe'''\n",
    "    return x.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "def return_mode_median_filled_int(x):\n",
    "    '''Return back series by filling with mode(in case there is one mode) else fill with integer part of median'''\n",
    "    modes = x.mode()\n",
    "    if len(modes) == 1:\n",
    "        return x.fillna(modes[0])\n",
    "    else:\n",
    "        return x.fillna(int(modes.median()))\n",
    "\n",
    "def return_mode_average_filled(x):\n",
    "    '''Return back series by filling with mode(in case there is one mode) else fill with average of modes'''\n",
    "    modes = x.mode()\n",
    "    if len(modes) == 1:\n",
    "        return x.fillna(modes[0])\n",
    "    else:\n",
    "        return x.fillna(modes.mean())\n",
    "\n",
    "def fill_month_history(x):\n",
    "    '''Return months filled data for 8-months period'''\n",
    "    first_non_null_idx = x.argmin()\n",
    "    first_non_null_value = x.iloc[first_non_null_idx]\n",
    "    return pd.Series(first_non_null_value + np.array(range(-first_non_null_idx, 8-first_non_null_idx)), index = x.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions For Neural Networks Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate predicted vs test data categorical variables\n",
    "def plot_prediction_vs_test_categorical(y_test, y_pred, class_labels):\n",
    "    # Plots the prediction vs test data for categorical variables.\n",
    "\n",
    "    # Args:\n",
    "    #     y_test (array-like): True labels of the test data.\n",
    "    #     y_pred (array-like): Predicted labels of the test data.\n",
    "    #     class_labels (list): List of class labels.\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Calculates performance of multivariate classification model\n",
    "def calculate_performance_multiclass(y_true, y_pred):\n",
    "    # Calculates various performance metrics for multiclass classification.\n",
    "\n",
    "    # Args:\n",
    "    #     y_true: The true labels.\n",
    "    #     y_pred: The predicted labels.\n",
    "\n",
    "    # Returns:\n",
    "    #     A dictionary containing the calculated metrics.\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Accuracy\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Precision, Recall, and F1-score (macro-averaged)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "    metrics['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns, data cleaning and correcting data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Ideally data cleaning should be done in parallel to discussions with domain expert to understand what values are appropriate in the columns, can they be retrieved if missing and do the columns depend upon each other. Unfortunately, such kind of support is not available in this kaggle project and therefore, we will deal with the data as per our understanding approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dataset info, many of the columns in our dataset have null values within them, representing missing values. Also, some columns are not of the correct data type as per the data they hold, this means there might be some textual characters within the data indicating unclean data and maybe placeholders which describe non-existing data or missing data and therefore, that is not getting captured as null values but as strings. We need to identify these values and first change them to null values before we do any further pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the columns one by one. Only columns which need some cleaning will be dealt with below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaning():\n",
    "    global df\n",
    "\n",
    "    print(\"Process (Clean) data:\")\n",
    "    print(\"*********************\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Cleaning start\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Performing Data Clean Up\")\n",
    "\n",
    "    # #### 1. Customer ID\n",
    "\n",
    "    # %%\n",
    "    df['Customer_ID'].unique()\n",
    "\n",
    "    # %%\n",
    "    df['Customer_ID'].nunique()\n",
    "\n",
    "    df['Customer_ID'].str.contains('CUS_0x').value_counts()\n",
    "\n",
    "    # #### 2. Name\n",
    "    df.drop(columns = ['Name'], inplace = True)\n",
    "\n",
    "    # #### 3. Age\n",
    "    df['Age'][~df['Age'].str.isnumeric()].unique() #extracting non-numeric textual data\n",
    "    df['Age'] = df['Age'].str.replace('_', '')\n",
    "    df['Age'][~df['Age'].str.isnumeric()].unique()\n",
    "    df['Age'] = df['Age'].astype(int)\n",
    "\n",
    "    # #### 4. SSN\n",
    "    df.drop(columns = ['SSN'], inplace = True)\n",
    "\n",
    "    # #### 5. Occupation\n",
    "    df['Occupation'][df['Occupation'] == '_______'] = np.nan\n",
    "\n",
    "    # #### 6. Annual Income\n",
    "    df['Annual_Income'][~df['Annual_Income'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique() # using regex to find values which don't follow the patern of a float\n",
    "    df['Annual_Income'] = df['Annual_Income'].str.replace('_', '')\n",
    "    df['Annual_Income'][~df['Annual_Income'].str.fullmatch('([0-9]*[.])?[0-9]+')]\n",
    "    df['Annual_Income'] = df['Annual_Income'].astype(float)\n",
    "\n",
    "    # #### 7. Number of Loans\n",
    "\n",
    "    df['Num_of_Loan'][~df['Num_of_Loan'].str.isnumeric()].unique()\n",
    "    df['Num_of_Loan'] = df['Num_of_Loan'].str.replace('_', '').astype(int)\n",
    "\n",
    "    # #### 8. Number of delayed payments\n",
    "    temp_series = df['Num_of_Delayed_Payment'][df['Num_of_Delayed_Payment'].notnull()]\n",
    "\n",
    "    temp_series[~temp_series.str.isnumeric()].unique()\n",
    "    df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].str.replace('_', '').astype(float)\n",
    "\n",
    "    # #### 9. Changed Credit Limit\n",
    "    df['Changed_Credit_Limit'][~df['Changed_Credit_Limit'].str.fullmatch('[+-]?([0-9]*[.])?[0-9]+')].unique()\n",
    "    df['Changed_Credit_Limit'][df['Changed_Credit_Limit'] == '_'] = np.nan \n",
    "    df['Changed_Credit_Limit'] = df['Changed_Credit_Limit'].astype(float)\n",
    "\n",
    "    # #### 10. Credit Mix\n",
    "    df['Credit_Mix'][df['Credit_Mix'] == '_'] = np.nan\n",
    "\n",
    "    # #### 11. Outstanding debt\n",
    "    df['Outstanding_Debt'][~df['Outstanding_Debt'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique()\n",
    "    df['Outstanding_Debt'] = df['Outstanding_Debt'].str.replace('_', '')\n",
    "    df['Outstanding_Debt'][~df['Outstanding_Debt'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique()\n",
    "    df['Outstanding_Debt'] = df['Outstanding_Debt'].astype(float)\n",
    "    \n",
    "    # #### 12. Amount Invested Monthly\n",
    "    temp_series = df['Amount_invested_monthly'][df['Amount_invested_monthly'].notnull()]\n",
    "    temp_series[~temp_series.str.fullmatch('([0-9]*[.])?[0-9]+')].unique()\n",
    "    df['Amount_invested_monthly'] = df['Amount_invested_monthly'].str.replace('_', '').astype(float)\n",
    "\n",
    "    # #### 13. Payment Behaviour\n",
    "    df['Payment_Behaviour'][df['Payment_Behaviour'] == '!@9#%8'] = np.nan\n",
    "\n",
    "    # #### 14. Monthly Balance\n",
    "    temp_series = df['Monthly_Balance'][df['Monthly_Balance'].notnull()]\n",
    "    temp_series[temp_series.str.fullmatch('[+-]*([0-9]*[.])?[0-9]+') == False].unique()\n",
    "    df['Monthly_Balance'][df['Monthly_Balance'] == '__-333333333333333333333333333__'] = np.nan\n",
    "    df['Monthly_Balance'] = df['Monthly_Balance'].astype(float)\n",
    "    df['Month'] = df['Month'].map({'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6, 'July': 7, 'August': 8})\n",
    "    df.sort_values(by = ['Customer_ID', 'Month'], ignore_index = True, inplace = True)\n",
    "    df.drop(columns = 'ID', inplace = True)\n",
    "    df.head(8)\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # #### 1. Age \n",
    "    df['Age'][(df['Age'] > 100) | (df['Age'] <= 0)] = np.nan \n",
    "    summary_age = summarize_numerical_column_with_deviation(df, 'Age', median_standardization_summary = True)\n",
    "    df['Age'][df.groupby('Customer_ID')['Age'].transform(median_standardization, default_value = return_max_MAD(df, 'Age')) > 80] = np.nan\n",
    "    df['Age'] =  df.groupby('Customer_ID')['Age'].transform(forward_backward_fill).astype(int)\n",
    "    df.groupby('Customer_ID')['Age'].nunique().value_counts()\n",
    "\n",
    "    df.groupby('Customer_ID')['Age'].agg(validate_age).value_counts()\n",
    "\n",
    "    # #### 2. Occupation \n",
    "    df['Occupation'].isnull().sum()\n",
    "    df.groupby('Customer_ID')['Occupation'].nunique().value_counts()\n",
    "    df.groupby('Customer_ID')['Occupation'].count().value_counts()\n",
    "    df['Occupation'] = df.groupby('Customer_ID')['Occupation'].transform(forward_backward_fill)\n",
    "    df['Occupation'].isnull().sum()\n",
    "\n",
    "    # #### 3. Annual Income and monthly inhand salary\n",
    "    summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)\n",
    "\n",
    "    summary_monthly_inhand_salary = summarize_numerical_column_with_deviation(df, 'Monthly_Inhand_Salary', 'Customer_ID', True, True)\n",
    "\n",
    "    df.groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_num_of_modes).value_counts()\n",
    "    df[df.groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_num_of_modes) == 2]\n",
    "    df['Annual_Income'][df['Monthly_Inhand_Salary'].notnull()] = df[df['Monthly_Inhand_Salary'].notnull()].groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_mode)\n",
    "\n",
    "    summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)\n",
    "\n",
    "    df['Monthly_Inhand_Salary'] = df.groupby(['Customer_ID', 'Annual_Income'], group_keys = False)['Monthly_Inhand_Salary'].transform(forward_backward_fill)\n",
    "    df['Monthly_Inhand_Salary'].isnull().sum()\n",
    "    Annual_Income_deviation = df.groupby('Customer_ID', group_keys = False)['Annual_Income'].apply(lambda x: (x - x.median())/x.median())\n",
    "    temp = Annual_Income_deviation[df['Monthly_Inhand_Salary'].isnull()]\n",
    "\n",
    "    # print(temp.describe())\n",
    "    df['Annual_Income'][df['Monthly_Inhand_Salary'].isnull()] = np.nan\n",
    "    Annual_Income_deviation = df.groupby('Customer_ID', group_keys = False)['Annual_Income'].apply(lambda x: (x - x.median())/x.median())\n",
    "    Annual_Income_deviation[Annual_Income_deviation > 500]\n",
    "    df.iloc[[34042]]\n",
    "    df[df['Customer_ID'].isin(['CUS_0x6079'])]\n",
    "\n",
    "    df.loc[[34042], ['Annual_Income', 'Monthly_Inhand_Salary']] = np.nan\n",
    "\n",
    "    df['Annual_Income'] = df.groupby('Customer_ID')['Annual_Income'].transform(forward_backward_fill)\n",
    "    df['Monthly_Inhand_Salary'] = df.groupby('Customer_ID')['Monthly_Inhand_Salary'].transform(forward_backward_fill)\n",
    "    summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)\n",
    "    summary_monthly_inhand_salary = summarize_numerical_column_with_deviation(df, 'Monthly_Inhand_Salary', 'Customer_ID', True, False)\n",
    "    \n",
    "    # #### 4. Number of Bank Accounts\n",
    "    summary_num_bank_accounts = summarize_numerical_column_with_deviation(df, 'Num_Bank_Accounts', median_standardization_summary = True)\n",
    "    summary_num_bank_accounts\n",
    "\n",
    "    df['Num_Bank_Accounts'][df['Num_Bank_Accounts'] < 0] = np.nan\n",
    "    df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts')).value_counts()\n",
    "    np.sort((df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts'))).unique())[:10]\n",
    "    df['Num_Bank_Accounts'][df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts')).abs() > 2] = np.nan\n",
    "    summary_num_bank_accounts = summarize_numerical_column_with_deviation(df, 'Num_Bank_Accounts', median_standardization_summary = True)\n",
    "    df['Num_Bank_Accounts'] = df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(forward_backward_fill).astype(int)\n",
    "    df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "    df.groupby('Customer_ID')['Num_Bank_Accounts'].agg(lambda x: x.diff().sum()).value_counts()\n",
    "\n",
    "    # #### 5. Number of credit cards\n",
    "    summary_num_credit_cards = summarize_numerical_column_with_deviation(df, 'Num_Credit_Card', median_standardization_summary = True)\n",
    "    summary_num_credit_cards\n",
    "    df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card')).value_counts()\n",
    "    np.sort((df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card'))).unique())[:10]\n",
    "\n",
    "    df['Num_Credit_Card'][df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card')).abs() > 2] = np.nan\n",
    "\n",
    "    summary_num_credit_cards = summarize_numerical_column_with_deviation(df, 'Num_Credit_Card', median_standardization_summary = True)\n",
    "    df['Num_Credit_Card'] = df.groupby('Customer_ID')['Num_Credit_Card'].transform(forward_backward_fill).astype(int)\n",
    "    df.groupby('Customer_ID')['Num_Credit_Card'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "    df.groupby('Customer_ID')['Num_Credit_Card'].agg(lambda x: x.diff().sum()).value_counts()\n",
    "\n",
    "    # #### 6. Interest Rate\n",
    "\n",
    "    # %%\n",
    "    summary_interest_rate = summarize_numerical_column_with_deviation(df, 'Interest_Rate', median_standardization_summary = True)\n",
    "\n",
    "    # %%\n",
    "    summary_interest_rate\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Interest_Rate'].nunique().value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # What we observe is MAD is 0(since max. MAD is 0) for each customer. Thus, it is hard to look at median standardization and assess points using this. Lets try to look at deviation from median. Since interest rate is not a feature whose median should deviate too much in scale from customer to customer.\n",
    "\n",
    "    # %%\n",
    "    deviation_from_median = df.groupby('Customer_ID')['Interest_Rate'].transform(lambda x: (x - x.median()))\n",
    "\n",
    "    # %%\n",
    "    deviation_from_median.describe()\n",
    "\n",
    "    # %%\n",
    "    deviation_from_median.value_counts()\n",
    "\n",
    "    # %%\n",
    "    np.sort(deviation_from_median.unique())\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Looking at the above data indicates that either interest rate is same as median or varies by at least 37% difference or more which is almost never seen in real life. We will fill all the records of customer with customer's median.\n",
    "\n",
    "    # %%\n",
    "    df['Interest_Rate'] = df.groupby('Customer_ID')['Interest_Rate'].transform(lambda x: x.median())\n",
    "\n",
    "    # %%\n",
    "    df['Interest_Rate'].describe()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 7. Number of loans\n",
    "\n",
    "    # %%\n",
    "    summary_num_of_loans = summarize_numerical_column_with_deviation(df, 'Num_of_Loan')\n",
    "\n",
    "    # %% [markdown]\n",
    "    # There are clearly some outliers as the number of loans can't be these many.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # The type of loans column can be used to extract this information accurately as the loans taken, their order and the count is embedded inside type of loan column.\n",
    "\n",
    "    # %%\n",
    "    df['Type_of_Loan'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # There are 11408 null values inside type of loan column which were added by us after replacing the placeholder. Maybe these represent no loans. Lets extract the loans count from type of loan column and fill it in num of loans column.\n",
    "\n",
    "    # %%\n",
    "    num_of_loans = df['Type_of_Loan'].str.split(', ').str.len()\n",
    "\n",
    "    # %%\n",
    "    df['Num_of_Loan'][num_of_loans.notnull()] = num_of_loans[num_of_loans.notnull()]\n",
    "\n",
    "    # %% [markdown]\n",
    "    # We have changed the values of number of loans columns for which we had valid data from type of loan column. Lets look at the values in number of loans column which were not touched.\n",
    "\n",
    "    # %%\n",
    "    df['Num_of_Loan'][num_of_loans.isnull()].value_counts()\n",
    "\n",
    "    # %%\n",
    "    np.sort(df['Num_of_Loan'][num_of_loans.isnull()].value_counts().index)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Mostly these are 0 which represent no loans. Other than this there are either negative values or too high values to be representing real count for number of loans. Looking at the data, we can assume here that all these are erroneous values and should actually be 0 i.e. specifying no loans.\n",
    "\n",
    "    # %%\n",
    "    df['Num_of_Loan'][num_of_loans.isnull()] = 0\n",
    "\n",
    "    # %%\n",
    "    df['Num_of_Loan'] = df.groupby('Customer_ID')['Num_of_Loan'].transform(forward_backward_fill).astype(int)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # What if we take one level difference at customer level? The difference can be 0, negative or positive but shouldn't be too high.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Num_of_Loan'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # This means that number of loans remain same throughout the 8-months period for each customer.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 8. Type of loan\n",
    "\n",
    "    # %%\n",
    "    df['Type_of_Loan'].value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Here we see that the placeholder 'Not Specified' has been used as a way of indicating that the type of loan has not been specified by the customer. \n",
    "\n",
    "    # %%\n",
    "    df['Type_of_Loan'].nunique()\n",
    "\n",
    "    # %%\n",
    "    df['Type_of_Loan'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Total 11408 null values. As noted earlier with number of loans these most probably represent no loans.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # We can replace the same with our own placeholder for that - 'No Loan'.\n",
    "\n",
    "    # %%\n",
    "    df['Type_of_Loan'].fillna('No Loan', inplace = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Lets seen what and how many unique type of loans we have.\n",
    "\n",
    "    # %%\n",
    "    temp_series = df['Type_of_Loan']\n",
    "\n",
    "    # %%\n",
    "    temp_lengths = temp_series.str.split(', ').str.len().astype(int) # Number of loans\n",
    "\n",
    "    # %%\n",
    "    temp_lengths_max = temp_lengths.max()\n",
    "\n",
    "    # %%\n",
    "    for index, val in temp_lengths.items():\n",
    "        temp_series[index] = (temp_lengths_max - val) * 'No Loan, ' + temp_series[index]\n",
    "\n",
    "    # %%\n",
    "    temp_series.head()\n",
    "\n",
    "    # %%\n",
    "    temp = temp_series.str.split(pat = ', ', expand = True)\n",
    "    unique_loans = set()\n",
    "    for col in temp.columns:\n",
    "        temp[col] = temp[col].str.lstrip('and ')\n",
    "        unique_loans.update(temp[col].unique())\n",
    "    # print(unique_loans)\n",
    "\n",
    "    # %%\n",
    "    len(unique_loans)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # There are total 8 unique type of loans, one placeholder for no specification of loan type and one placeholder added by us to specify there is no loan.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # When we are working with tree based models usually they don't need the categorical columns to be encoded to numerical data type like we need for linear regression, logistic regression etc. as the model can handle these. But scikit learn uses CART algorithm and there is no functionality of using categorical variables directly and thus, for modelling with scikit-learn they need to be numerically encoded. Right now in this column we have 6260 unique categories which is too much to search for at one node. as at each node, a decision tree will look at all possible values of the categorical column to figure out what value produces the best split based on gini impurity or entropy decrease on splitting. We can do some pre-prcoessing on this column to split it into multiple columns. Intutively, it feels like the lastest loan should have high influence on your credit score because if the loan is heavy in nature then it might lead to poor credit score if unable to pay while if it is light then the credit score should remain almost same as before. We can split the column in following format: Latest loan1, latest loan2 etc. This way we will have 9 columns corresponding to maximum number of loans for any customer and each column can have maximum 10 categorical values corresponding to the unqiue loans calculated above. \n",
    "\n",
    "    # %% [markdown]\n",
    "    # This approach will have following benefits:  \n",
    "    # 1. Preserves the order of loans even after splitting.\n",
    "    # 2. Easier to visualize and understand patterns since number of categories per column reduces.    \n",
    "    # 3. Algorithm can focus more on information contained within the loan sequence. For example, if second last and third last loan contain critical information in classifying credit score than focussing on whole sequence of loans is not worthwhile and this inturn might lead to smaller decision trees and faster training compared to if we didn't split.  \n",
    "    # 4. At each node for 9 of the splitted columns only total 90(9 columns * 10 categories) comparisons need to be made after one-hot encoding rather than 6260 comparisons for one non-splitted column.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Another possible approach could be to split columns as first loan, second loan etc. as a way of splitting but the pre-processing mentioned above feels more effective for now. We will maybe look at these two modeling startegies whn we do EDA and modeling.\n",
    "\n",
    "    # %%\n",
    "    temp.columns = [f'Last_Loan_{i}' for i in range(int(df['Num_of_Loan'].max()), 0, -1)]\n",
    "\n",
    "    # %%\n",
    "    temp.head()\n",
    "\n",
    "    # %%\n",
    "    df = pd.merge(df, temp, left_index = True, right_index = True)\n",
    "\n",
    "    # %%\n",
    "    df.head()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # We can drop the type of loan column from dataset.\n",
    "\n",
    "    # %%\n",
    "    df.drop(columns = 'Type_of_Loan', inplace = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 9. Delay from due date\n",
    "\n",
    "    # %%\n",
    "    summary_due_date = summarize_numerical_column_with_deviation(df, 'Delay_from_due_date', median_standardization_summary = True)\n",
    "\n",
    "    # %%\n",
    "    summary_due_date\n",
    "\n",
    "    # %% [markdown]\n",
    "    # The median standardization varies quite a bit going from -10 to 11. \n",
    "\n",
    "    # %%\n",
    "    due_date_deviation = df.groupby('Customer_ID')['Delay_from_due_date'].transform(median_standardization, default_value = return_max_MAD(df, 'Delay_from_due_date'))\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Looking at the fact that overall distribution of delay from due date is not too extreme and delay from due date can vary a lot as well unlike number of credit cards or number of bank accounts. We will move forward with the data as it is. Having a domain expert by your side would have helped make this more clearer.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 10. Number of delayed payments\n",
    "\n",
    "    # %%\n",
    "    summary_num_delayed_payments = summarize_numerical_column_with_deviation(df, 'Num_of_Delayed_Payment', median_standardization_summary = True)\n",
    "\n",
    "    # %%\n",
    "    summary_num_delayed_payments\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Judging from median standardization, almost all of the values are same as median and this is leading to 0 median standardization. Median standardization should definitely should be like this and should be skewed in nature but its hard to assess what threshold to use without a domain expert. We will use the full column as a sample and judge based on that here.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # The number of delayed payments can't be too much and can not be negative as well. We will set negative values and values greater than upper range of oultiers to null.\n",
    "\n",
    "    # %%\n",
    "    df['Num_of_Delayed_Payment'][(df['Num_of_Delayed_Payment'] > summary_num_delayed_payments['Num_of_Delayed_Payment']['Outlier upper range']) | (df['Num_of_Delayed_Payment'] < 0)] = np.nan\n",
    "\n",
    "    # %%\n",
    "    df['Num_of_Delayed_Payment'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # There are 8382 null values. Lets observe the count of diff in between consecutive months and observe if we can identify some pattern exising there which can help us make some educated guess about the null values.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(pd.Series.diff).value_counts(normalize = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Around 45.6% of time it remains same across months but rest of the time it varies i.e more than 50% of the time it varies across months.\n",
    "\n",
    "    # %%\n",
    "    df[['Customer_ID', 'Num_of_Delayed_Payment']].head(40)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Looking at the data it looks like usually a single value repeats more often across months i.e. mode might be a suitable choice here. But first lets see that usually how many times the mode occurs for any customer.\n",
    "\n",
    "    # %%\n",
    "    # Ratio of frequency of mode and number of non-null data per customer\n",
    "    temp = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(lambda x: (x == x.mode()[0]).sum()/x.notnull().sum()).value_counts(normalize = True)\n",
    "\n",
    "    # %%\n",
    "    temp[temp.index > 0.5].sum() # Idenitfying how many times the mode occurs in more than 50% of non-null data per customer\n",
    "\n",
    "    # %% [markdown]\n",
    "    # That is within given data for around 75.8% of the customers the mode occurs more than 50% of the time within 8-months period for whatever data we have available. This means the mode might be a suitable imputation here.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # What if there are multiple modes per customer? Lets check the data if such thing exists.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Num_of_Delayed_Payment'].agg(lambda x: len(x.mode())).value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Mostly, we observe one mode but sometimes it can be more than one as well. What to do in multiple modes case? We can take some average or median of modes in that case, in case there is skewness within the modes, median would be a better guess and in case where medians come out to be floating point number we can just take the integer part as an approximation.\n",
    "\n",
    "    # %%\n",
    "    df['Num_of_Delayed_Payment'] = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(return_mode_median_filled_int).astype(int)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Number of delayed payments is something that can vary from month to month its not like it has monotically increasing pattern or so which we can use as a sanity check. What we can try to observe is the relative devaition from median for this cleaned column.\n",
    "\n",
    "    # %%\n",
    "    summarize_numerical_column_with_deviation(df, 'Num_of_Delayed_Payment', median_standardization_summary = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 11. Changed credit limit\n",
    "\n",
    "    # %%\n",
    "    summary_changed_credit_limit = summarize_numerical_column_with_deviation(df, 'Changed_Credit_Limit', median_standardization_summary = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Credit card limit is dependent upon the users usage patterns. If the lender trusts the customer then it can increase also and if customer is late on payments, low activity etc. then the credit lmit can decrease as well. Thus, both negative and positive values are understandable.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # The upper range for outliers for full column doesn't significantly deviate from the max value and its difficult to judge here what threshold should be placed on credit limit median standardization. Thus, we leave non-null values as it is for now.\n",
    "\n",
    "    # %%\n",
    "    df[['Customer_ID', 'Changed_Credit_Limit']].head(40)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Looking at the data usually the credit limit occurs with the same value across months i.e. the mode might be an appropriate value to imputate. Lets do some checks first though.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Changed_Credit_Limit'].agg(lambda x: len(x.mode())).value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Almost all the time only one mode appears. But sometimes two mode can occur as well, since this is a floating point type feature and there are only two mode values we will choose average of both which will be same as median in this case.\n",
    "\n",
    "    # %%\n",
    "    df['Changed_Credit_Limit'] = df.groupby('Customer_ID')['Changed_Credit_Limit'].transform(return_mode_average_filled)\n",
    "\n",
    "    # %%\n",
    "    df['Changed_Credit_Limit'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 12. Number of credit card inquiries\n",
    "\n",
    "    # %%\n",
    "    summary_num_credit_inquiries = summarize_numerical_column_with_deviation(df, 'Num_Credit_Inquiries', median_standardization_summary = True)\n",
    "    df['Num_Credit_Inquiries'][(df['Num_Credit_Inquiries'] > summary_num_credit_inquiries['Num_Credit_Inquiries']['Outlier upper range']) | (df['Num_Credit_Inquiries'] < 0)] = np.nan\n",
    "\n",
    "    # %%\n",
    "    df['Num_Credit_Inquiries'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Lets look at some data.\n",
    "\n",
    "    # %%\n",
    "    df[['Customer_ID', 'Num_Credit_Inquiries']].head(40)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # This a type of data which is monotically increasing in nature and thus should increase or remain same as months go on. Lets check that.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # As expected it mostly either remains same or increases. In this case we can just use forward fill and backward fill to fill these nulls.\n",
    "\n",
    "    # %%\n",
    "    df['Num_Credit_Inquiries'] = df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(forward_backward_fill).astype(int)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Lets do the check again.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 13. Credit Mix\n",
    "    df['Credit_Mix'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # A lot of null values present. We have already seen that during the 8-months period the type of loans and number of loans remain same so its fair to assume credit mix will also remain same. Lets check that with the given data.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Credit_Mix'].nunique().value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # We can observe that one customer has only one type of credit mix only, throughout the 8-months period apart from null values. We can just use forward fill and bacward fill to achieve the desired goal.\n",
    "\n",
    "    # %%\n",
    "    df['Credit_Mix'] = df.groupby('Customer_ID')['Credit_Mix'].transform(forward_backward_fill)\n",
    "\n",
    "    # %%\n",
    "    df['Credit_Mix'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 14. Outstanding debt\n",
    "\n",
    "    # %%\n",
    "    summary_outstanding_debt = summarize_numerical_column_with_deviation(df, 'Outstanding_Debt', median_standardization_summary = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # All the values in median standardization are coming out to be zero. Is the outstanding debt constant for each customer across months after ignoring nulls?\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Outstanding_Debt'].nunique().value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # The column looks ok from the distribution perspective and there are no nulls present.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 15. Credit Utilization ratio\n",
    "\n",
    "    # %%\n",
    "    summary_credit_utilization_ratio = summarize_numerical_column_with_deviation(df, 'Credit_Utilization_Ratio', median_standardization_summary = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Judging from both the graphs its hard to put a threshold on median standardization of credit utilization ratio without a domin expert and also, the distribution of the column as a whole looks decent enough to not touch it further.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 16. Credit History Age\n",
    "\n",
    "    # %%\n",
    "    df[['Customer_ID', 'Credit_History_Age']].head(40)\n",
    "\n",
    "    # %%\n",
    "    df['Credit_History_Age'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # There are 9030 null values. Looking at the data it is of the format - '{Year} Years and {Months} Months'. Using str functions of pandas series, we can extract these two data values i.e. year and months. We will then combine them in a single column as total months because both the year data and month data can be easily extracted from total months so there will be no loss of information and we will be able to reduce one feature from our dataset.\n",
    "\n",
    "    # %%\n",
    "    df[['Years', 'Months']] = df['Credit_History_Age'].str.extract('(?P<Years>\\d+) Years and (?P<Months>\\d+) Months').astype(float)\n",
    "    df[['Years', 'Months']].describe()\n",
    "    df['Credit_History_Age'] = df['Years'] * 12 + df['Months']\n",
    "\n",
    "    # %%\n",
    "    df.drop(columns = ['Years', 'Months'], inplace = True)\n",
    "\n",
    "    df['Credit_History_Age'].isnull().sum()\n",
    "    df['Credit_History_Age'] = df.groupby('Customer_ID')['Credit_History_Age'].transform(fill_month_history).astype(int)\n",
    "\n",
    "    # #### 17. Payment of minimum amount\n",
    "    df['Payment_of_Min_Amount'].value_counts()\n",
    "\n",
    "    df.groupby(['Customer_ID'])['Payment_of_Min_Amount'].nunique().value_counts()\n",
    "\n",
    "    df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map({'Yes': 1, 'No': 0, 'NM': np.nan})\n",
    "\n",
    "    df.groupby('Customer_ID')['Payment_of_Min_Amount'].transform(pd.Series.diff).value_counts()\n",
    "\n",
    "    df['Payment_of_Min_Amount'] = df.groupby('Customer_ID')['Payment_of_Min_Amount'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    df['Payment_of_Min_Amount'].isnull().sum()\n",
    "    df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map({1: 'Yes', 0: 'No'})\n",
    "\n",
    "    # #### 18. Total EMI per month\n",
    "    summary_total_emi_per_month = summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)\n",
    "    summary_total_emi_per_month\n",
    "\n",
    "    deviation_total_emi = df.groupby('Customer_ID')['Total_EMI_per_month'].transform(median_standardization, default_value = return_max_MAD(df, 'Total_EMI_per_month'))\n",
    "\n",
    "    df['Total_EMI_per_month'][deviation_total_emi > 10000] = np.nan\n",
    "    summary_total_emi_per_month = summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)\n",
    "    df['Total_EMI_per_month'][(df['Total_EMI_per_month'] > summary_total_emi_per_month['Total_EMI_per_month']['Outlier upper range'])] = np.nan\n",
    "    df['Total_EMI_per_month'].isnull().sum()\n",
    "    df.groupby('Customer_ID')['Total_EMI_per_month'].nunique().value_counts()\n",
    "    deviation_total_emi = df_copy.groupby('Customer_ID', group_keys = False)['Total_EMI_per_month'].transform(median_standardization, default_value = return_max_MAD(df_copy, 'Total_EMI_per_month'))\n",
    "\n",
    "    # %%\n",
    "    temp = (deviation_total_emi[df.groupby('Customer_ID')['Total_EMI_per_month'].transform(pd.Series.nunique) == 0])\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Looking at this, only one value looks absurdly big. The rest of the median standardization's could even be considered ok for now.\n",
    "\n",
    "    # %%\n",
    "    temp[temp > 80]\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Lets convert this value to null and feed it back to the dataset.\n",
    "\n",
    "    # %%\n",
    "    temp[79370] = np.nan\n",
    "    df['Total_EMI_per_month'][temp.index] = temp\n",
    "\n",
    "    # %%\n",
    "    summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Now the data looks more appropriate compared to before. There are still 4420 null values which need to be handled here.\n",
    "\n",
    "    # %% [markdown]\n",
    "    # The rest of the null values can be filled using forward and backward fill as the EMI's should be highly dependent upon previous month.\n",
    "\n",
    "    # %%\n",
    "    df['Total_EMI_per_month'] = df.groupby('Customer_ID')['Total_EMI_per_month'].transform(forward_backward_fill)\n",
    "\n",
    "    # %%\n",
    "    df['Total_EMI_per_month'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 19. Amount Invested Monthly\n",
    "\n",
    "    # %%\n",
    "    summary_amount_invested_monthly = summarize_numerical_column_with_deviation(df, 'Amount_invested_monthly', median_standardization_summary = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Some values in amount invested monthly are too extreme compared to the rest of the data and thus, can be removed considering them to be erroneous before we do further processing.\n",
    "\n",
    "    # %%\n",
    "    df['Amount_invested_monthly'][df['Amount_invested_monthly'] > 8000] = np.nan\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Lets check the distribution again.\n",
    "\n",
    "    # %%\n",
    "    summary_amount_invested_monthly = summarize_numerical_column_with_deviation(df, 'Amount_invested_monthly', median_standardization_summary = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Looks like power law distribution, hopefully these are not erroneous values. Lets leave these non-null values as it is for now. Null values still need to be handled.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Amount_invested_monthly'].transform(return_num_of_modes).value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Lets choose the median of values as a decent approximation for null values.\n",
    "\n",
    "    # %%\n",
    "    df['Amount_invested_monthly'] = df.groupby('Customer_ID')['Amount_invested_monthly'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 20. Payment Behaviour\n",
    "    df['Payment_Behaviour'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # 7600 of null values present.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Payment_Behaviour'].nunique().value_counts()\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Payment_Behaviour'].agg(return_num_of_modes).value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # The number of modes vary, if the number of mode is 1 then we can use that for imputation else forward fill and backward fill can be used.\n",
    "\n",
    "    # %%\n",
    "    df['Payment_Behaviour'] = df.groupby('Customer_ID')['Payment_Behaviour'].transform(lambda x: return_mode(x) if len(x.mode()) == 1 else forward_backward_fill(x))\n",
    "\n",
    "    # %%\n",
    "    df['Payment_Behaviour'].isnull().sum()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # #### 21. Monthly Balance\n",
    "\n",
    "    # %%\n",
    "    summary_monthly_balance = summarize_numerical_column_with_deviation(df, 'Monthly_Balance', median_standardization_summary = True)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # There are 1209 null values. Looking at the column as a whole the distribution looks ok and considering the fact that we can't decide exactly on a threshold on median standardization without domain expertise. We will leave the non-null values as it for now.\n",
    "\n",
    "    # %%\n",
    "    df.groupby('Customer_ID')['Monthly_Balance'].nunique().value_counts()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Since there might be skewness within the data we can use median to fill the null values.\n",
    "\n",
    "    # %%\n",
    "    df['Monthly_Balance'] = df.groupby('Customer_ID')['Monthly_Balance'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ### Deleting unnecessary columns\n",
    "\n",
    "    # %%\n",
    "    df.columns\n",
    "\n",
    "    # %% [markdown]\n",
    "    # Month column is not needed anymore and can be dropped. We will keep customer id as it is for now so that it can be used later on when doing train-test splits.\n",
    "\n",
    "    # %%\n",
    "    df.drop(columns = ['Month'], inplace = True)\n",
    "\n",
    "    # %%\n",
    "    df = df.sample(frac = 1) #shuffle data\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ### Rearranging the columns\n",
    "\n",
    "    # %%\n",
    "    df.columns\n",
    "\n",
    "    # %%\n",
    "    df = df.loc[:, ['Customer_ID', 'Age', 'Occupation', 'Annual_Income',\n",
    "        'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
    "        'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
    "        'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n",
    "        'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt',\n",
    "        'Credit_Utilization_Ratio', 'Credit_History_Age',\n",
    "        'Payment_of_Min_Amount', 'Total_EMI_per_month',\n",
    "        'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance', 'Last_Loan_9', 'Last_Loan_8', 'Last_Loan_7',\n",
    "        'Last_Loan_6', 'Last_Loan_5', 'Last_Loan_4', 'Last_Loan_3',\n",
    "        'Last_Loan_2', 'Last_Loan_1',\n",
    "        'Credit_Score']]\n",
    "\n",
    "    # Total rows after cleaning\n",
    "    total_rows_after_cleaning = len(df)\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Total Rows after cleaning is: {total_rows_after_cleaning}\")\n",
    "\n",
    "    # Export the cleaned data\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    file__out_path = os.path.join(parent_dir, \"data\\Credit_score_cleaned_data.csv\")\n",
    "    df.to_csv(file__out_path, index = False)\n",
    "\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(f\"\\nTime to process is: {processing_time:.2f} seconds\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Customer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Customer_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Customer_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the customer IDs follow a general trend of something like CUS_0x and then followed by some characters which uniquely define the ID. Lets check if all the values are like this or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Customer_ID'].str.contains('CUS_0x').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the records follow the same pattern for customer IDs and there is no missing information or placeholders here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name column is of no use as names shouldn't contain any pattern related to credit score classification. It has null values but we won't deal with them as the column itself is not needed. We have customer ID as a means of uniquely identifying the customers and thus, customer name can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns = ['Name'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column is of object data type(string) while it should be a column containing only natural number usually in between 0-100, that means there should be some placeholders or errors in data entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Age'][~df['Age'].str.isnumeric()].unique() #extracting non-numeric textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above values, looks like many underscores are present in our dataset. For age, they are not needed and can be replaced with blanks. Some negative values are also present which we will handle later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Age'] = df['Age'].str.replace('_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Age'][~df['Age'].str.isnumeric()].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By replacing underscores, the ambiguity in age column has disappeared. But as noticed earlier there are some erroneous values in age column which are too big to be considered as normal age like 6921, 1248 etc, we will deal with these later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Age'] = df['Age'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. SSN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social security number is another unique identifier and should not convey any particular information regarding credit score classification and thus, can be dropped as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns = ['SSN'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column looks ok in terms of the data type and also has no nulls. Lets try to look at the data distribution to make sure no placeholders are present for missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Occupation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a placeholder present. We will replace it with null for now and deal with it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Occupation'][df['Occupation'] == '_______'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Occupation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Annual Income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annual income should be positive float in nature but is of object data type(string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Annual_Income'][~df['Annual_Income'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique() # using regex to find values which don't follow the patern of a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of float values which have underscore in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Annual_Income'] = df['Annual_Income'].str.replace('_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Annual_Income'][~df['Annual_Income'].str.fullmatch('([0-9]*[.])?[0-9]+')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no problematic values present anymore. We can convert the column to type float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Annual_Income'] = df['Annual_Income'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Number of Loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column is of object data type(string) rather than non-negative integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Num_of_Loan'][~df['Num_of_Loan'].str.isnumeric()].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underscores are present at the end. For now, we will remove them and convert the column to type int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Num_of_Loan'] = df['Num_of_Loan'].str.replace('_', '').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Number of delayed payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column should be a non-negative integer but has object data type(string). This column already has some null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting non-null data\n",
    "#temp_series = df['Num_of_Delayed_Payment'][df['Num_of_Delayed_Payment'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_series[~temp_series.str.isnumeric()].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are values with underscore and negative signs. For now, we will replace underscores with blanks and convert to float data type. Negatives will be handled later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].str.replace('_', '').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Changed Credit Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column should be of type float in nature but is of object data type(string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Changed_Credit_Limit'][~df['Changed_Credit_Limit'].str.fullmatch('[+-]?([0-9]*[.])?[0-9]+')].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are underscores as placeholders. We can change them to null values and column data type to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Changed_Credit_Limit'][df['Changed_Credit_Limit'] == '_'] = np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Changed_Credit_Limit'] = df['Changed_Credit_Limit'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Credit Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Credit_Mix'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column contains underscores as placeholders. We will replace them with null for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Credit_Mix'][df['Credit_Mix'] == '_'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Outstanding debt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column should be of type non-negative float in nature but is of object data type(string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Outstanding_Debt'][~df['Outstanding_Debt'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a lot of values have underscore in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Outstanding_Debt'] = df['Outstanding_Debt'].str.replace('_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Outstanding_Debt'][~df['Outstanding_Debt'].str.fullmatch('([0-9]*[.])?[0-9]+')].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Outstanding_Debt'] = df['Outstanding_Debt'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Amount Invested Monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column should be of non-negative float type in nature but is of object data type(string). The column already consists null values within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting non-null data\n",
    "#temp_series = df['Amount_invested_monthly'][df['Amount_invested_monthly'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_series[~temp_series.str.fullmatch('([0-9]*[.])?[0-9]+')].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one value with underscores in it. We will remove these and convert data type to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Amount_invested_monthly'] = df['Amount_invested_monthly'].str.replace('_', '').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Payment Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Payment_Behaviour'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one value that is very weird and does not make sense. We will replace it with nulls for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Payment_Behaviour'][df['Payment_Behaviour'] == '!@9#%8'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Monthly Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column should be of float type in nature but is of object data type(string). Also, there are pre-existing null values in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting non-null data\n",
    "#temp_series = df['Monthly_Balance'][df['Monthly_Balance'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_series[temp_series.str.fullmatch('[+-]*([0-9]*[.])?[0-9]+') == False].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one value with underscore and its too big to be an actual value, we will replace it with null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Monthly_Balance'][df['Monthly_Balance'] == '__-333333333333333333333333333__'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Monthly_Balance'] = df['Monthly_Balance'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers and nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The idea behind data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree based models have the ability to handle null data and non-null data both. Looking at the given data, we have chronological data of 8-months period for all customers. It makes more sense to use this chronological order data to extract approximately accurate information to fill in the null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we can make decent guesses about missing information in this dataset, it will be better to go ahead with correction of nulls. If there are some null values about which we can't make any guess at all then we will leave those records as it is and that can handled by the tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For categorical columns:**  \n",
    "- We can find the prelevant patterns like repetition of a single mode, importance of chronological order etc. within the column and use that idea to make an educated guess about the missing values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For numerical columns:**  \n",
    "- Since there is a chronological order within the records, this can help us quantify the overall behaviour of customers and combine available data to make approximate guesses of missng data. Using deviation from median with respect to median absolute deviation(MAD) as a measure of a standardized behavioural pattern within 8-months period for a customer we can do so. After standardizing and combining these values for all customers, we can identify extreme values looking at from a behavioural persepctive rather than the exact values itself. If we suspect that these values are almost impossible to occur we can replace them with nulls.  \n",
    "- Once extreme values observed above are changed to nulls, then we can take the whole column as a sample and either rely on visualization, IQR scores or log transformation followed by IQR score to further assess any potentially problematic points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flagging potentially problematic outliers:**  \n",
    "The thing about these standard ranges of +/-3 sigma for z-score and +/-1.5 of IQR score being used for identifying outliers is that they do help us identify data points which don't follow the distribution as most of the points do but it does not mean at all that these flagged points outside these standard ranges are unreal or erroneous, they help us identify that these points might be problematic in nature and we need to further look into it. For example, in a highly skewed distribution like income distribution these measure would identify tail values as problematic but this does not mean they are unreal. Whether they follow the same pattern as rest of the data when coming to predictive modelling is totally a different case because for that we would need to visualize things with other predictor variables and target variable. For our data cleaning purpose, we will use these measures to identify potentially problmeatic points and then further assess these data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About deviation from median with respect to median absolute deviation(MAD):**  \n",
    "Also, known as median standardization, we expect that there will be skewness within the data and thats why this measure is used here as its robust to skewness. Our constructed measure uses the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(data value - median of customer)/Median absolute deviation of customer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be three scenarios with this equation:\n",
    "1. <u>When all values are same for a customer:</u> All non-null values are same as median then the function returns zero, indicating no deviation from median.\n",
    "2. <u> When MAD comes out to be zero but all values are not same:</u> Then the function uses maximum value of MAD out of all customers. This helps us quantify values which deviate from median via a conservative approach even though the MAD is zero for given customer. Dividing by xaro will lead to infintiy in numpy or pandas and we won't be able to differentiate in between small deviations and very huge deviations from median.   \n",
    "3. <u>When MAD is non-zero:</u> It simply returns the calculation from the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be two types of columns when we are trying to clean extreme values in the dataset:  \n",
    "1. Extremes of target column.  \n",
    "2. Extremes of predictor columns(X columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extremes of target column:**  \n",
    "1. <u>Classification:</u> There can be imbalance in the proportion of target variable classes and this might lead to poor predictions on the small classes if the algorithm is sensitive to imbalanced data. In that case, upsampling of minority classes, downsampling of majority classes, assinging bigger weights to minority classes , getting more data for minority classes or artificial data point generation techniques like SMOTE can be used.  \n",
    "2. <u>Regression:</u> If there is an extreme target value, then there can be two cases: either its erroneous and needs to be corrected or it is a legitimate value but is an extreme one compared to rest of the values. If its erroneous, then it needs to be handled in an appropriate way like dropping it, some sort of imputation, getting the value from the data source etc. If its a legitimate value, then there can be two case: either it follows the pattern of the rest of the data or it does not. If it follws the pattern and is extreme in y-value then it might be better to do some type of transformation which brings the y-values closer so that the algorithm does not focus too much on the extreme point and thus, deviate from general pattern. It it does not follow the pattern, then this point is called an outlier, it can be removed if the pattern that it belongs to is of no concern or will be very rare, if left there, it might affect your generalization and definitely the standard error in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extremes of predictor column:**  \n",
    "- Usually, the algorithms do not demand anything from the predictor variables like they should be normally distributed or there should be balance in between the classes. But still having these standard distributions can help improve performance but there is no strict need.  \n",
    "- When considering numerical columns, records with extreme values of predictor variables are known as influential data points, there y-values or target values would have higher influence on the model equation compared to other records which have non-extreme predictor values in case of a linear regression model. If they follow the general pattern then they are not that much of a concern but if they don't then there is a problem right there and these points need to be removed if the pattern that it belongs to is of no concern or will be very rare which we don't want to account for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying errorneous values is a bit easier, especially if you have domain experts by your side. But identifying outliers and influential points is tougher as it calls for looking at high dimensional data points which is very hard to do so. Either we can remove these suspected outliers and influential points if we are able to identify that these values represent rare occurences or patterns that we don't want to predict in real time or if we are not able to identify such thing its best to build predictive model on top of all the data and then observe if these data points create any issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, there is obviously a chronological structure within the dataset, lets convert the months to integer based values for ease of sorting. We are not concerned about the months column because what month it is should not have any influence on credit score. We will drop it later on once we have used it to fill other columns appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Month'] = df['Month'].map({'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6, 'July': 7, 'August': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets sort by customer ID and months so that the records are in a structured order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_values(by = ['Customer_ID', 'Month'], ignore_index = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the ID column as that is just for unique row identification without it the customer ID and month can together uniquely identify any record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns = 'ID', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep a copy of the data just in case we need it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the columns one by one to figure out how to handle each individual column the best way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Age "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already noted there are many extreme values present in age column which are unrealistic in nature. Lets convert any inappropriate value which is not at all possible like negative and high positive values above 100 to null for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Age'][(df['Age'] > 100) | (df['Age'] <= 0)] = np.nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_age = summarize_numerical_column_with_deviation(df, 'Age', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly some extreme values when we look at median standardization, 4 data points look very extreme compared to other ages and can be replaced with nulls for now as these deviations are unrealistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Age'][df.groupby('Customer_ID')['Age'].transform(median_standardization, default_value = return_max_MAD(df, 'Age')) > 80] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed all inappropriate values and replaced them with nulls. What we have in our dataset is customer data, if some month's age data is missing then we can simply refer the other months data of same customer to replace with an appropriate value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we don't have birth dates its not exactly always possible to determine the exact value to replace null with. For example, if a customer has 17, null, 18 as ages for three consecutive months, then we can't say whether the missing value will be 17 or 18. We can't replace with 17.5 also as that is a value that is not possible for age and might miss lead the algorithm into believing that these values can also exist. So, we will use forward fill and backward fill in any order to fill these null values as the best guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Age'] =  df.groupby('Customer_ID')['Age'].transform(forward_backward_fill).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more checks that can be done is that in this 8-month time period a customer can have either one age or maximum two, not more than that:  \n",
    "1. Unique count of ages for any customer should be either 1 or 2.\n",
    "2. The ages if two in count should be consecutive integers i.e. the difference would be 1 occuring one time during 8-months period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Age'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking validity of ages\n",
    "#df.groupby('Customer_ID')['Age'].agg(validate_age).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks ok logically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Occupation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occupation_count = df['Occupation'].value_counts()\n",
    "# sns.barplot(x = occupation_count.values, y = occupation_count.index, orient = 'h')\n",
    "# plt.xlabel('Count')\n",
    "# plt.ylabel('Occupation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Occupation'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does all the customers have same occupation or multiple throughout the 8-months period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Occupation'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Occupation'].count().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, no one has transitioned into new role or maybe its not mentioned and is coming as null. But these possibilities we obviously can't tell until and unless we get back to the customer. We need to make the most appropriate educated guess for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transitioning to new role or leaving jobs in a count of 12500 customers during 8-month period is definitley possible but will be a relatively lower amount. For now, the best educated guess would be to take the same profession throughout the 8-months period. We can use forward fill and backward fill here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Occupation'] = df.groupby('Customer_ID')['Occupation'].transform(forward_backward_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Occupation'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Annual Income and monthly inhand salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both annual income and monthly inhand salary go hand in hand and if there is some erroneous or missing value in one column then the other might have information on what might be the correct guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the distribution of annual income. Right now, we don't have any null values present in this column but maybe there are extreme or erroneous values present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are a lot of extreme values present, typically the annual income is definitely skewed but not by this much and these are therefore most probably present due to error while filling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at some customer data to understand this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(41)\n",
    "#df[df['Customer_ID'].isin(np.random.choice(df['Customer_ID'].unique(), 5))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, looks like in some places the annual income is wrongly given as a very huge value even though the monthly inhand salary is same as other months. Also, in some places the monthly inhand salary is missing while annual income is given either same or different than some other months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at distriubtion of monthly inhand salary now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_monthly_inhand_salary = summarize_numerical_column_with_deviation(df, 'Monthly_Inhand_Salary', 'Customer_ID', True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that monthly inhand salary is mostly same but sometimes changes, which looks normal as salary may change due to shift allowances based on months and shifts, promotions, appraisals, other type of allowances and based on varying tax deduction. But as noted earlier monthly inhand salary column has null values which need to be handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since monthly inhand salary looks correct, we can use the non-null data of monthly inhand salary and group it with customer id, then fill all the annual incomes with the mode of annual income. Lets hope the data doesn't contain multiple modes for each group by of customer id and monthly inhand salary value, we will check that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_num_of_modes).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are some monthly inhand salaries which have 2 modes, this is problematic. How do we decide which mode to use in this case? Lets look at the data as the number is relatively very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_num_of_modes) == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This double mode situation is happening for 8 records and what we can notice is that there are too extreme values which are not suitable as annual income. We can just choose the minimum mode in these cases where two modes exist for same monthly inhand salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Annual_Income'][df['Monthly_Inhand_Salary'].notnull()] = df[df['Monthly_Inhand_Salary'].notnull()].groupby(['Customer_ID', 'Monthly_Inhand_Salary'], group_keys = False)['Annual_Income'].transform(return_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has handled all records for which monthly inhand salary is not null. Lets look at the distribution of Annual Income again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still some values are very high but it has reduced from before. These high values should exist in records which have null as monthly inhand salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets take care of monthly inhand salary as well, in a similar way as annual income. We can replace null values by looking at nearby monthly inhand salary of a same annual income value for given customer. It might have happened that monthly inhand salary changed because of some tax changes even though the annual income remained same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Monthly_Inhand_Salary'] = df.groupby(['Customer_ID', 'Annual_Income'], group_keys = False)['Monthly_Inhand_Salary'].transform(forward_backward_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Monthly_Inhand_Salary'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still 117 records exist which have null values in monthly inhand salary and correpsonding annual income is different from other month data. These values might probably be erroneous values. Lets look at the deviation with respect to median for these annual incomes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual_Income_deviation = df.groupby('Customer_ID', group_keys = False)['Annual_Income'].apply(lambda x: (x - x.median())/x.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = Annual_Income_deviation[df['Monthly_Inhand_Salary'].isnull()]\n",
    "# sns.boxplot(temp.values)\n",
    "# plt.title('Boxplot of relative deviation wrt median')\n",
    "# plt.ylabel('Annual Income deviation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(temp.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum is 3 i.e. the given extreme annual incomes are around 4 times the median annual income of the customer, these all are very very rare cases and most probably are present due to error in data entry. We will remove these values and fill them by neighbour records using fills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Annual_Income'][df['Monthly_Inhand_Salary'].isnull()] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the annual income deviation to see if it is now free of outliers or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual_Income_deviation = df.groupby('Customer_ID', group_keys = False)['Annual_Income'].apply(lambda x: (x - x.median())/x.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(Annual_Income_deviation.values)\n",
    "# plt.title('Boxplot of relative deviation wrt median')\n",
    "# plt.ylabel('Annual Income deviation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still one value. Lets extract and see their customer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual_Income_deviation[Annual_Income_deviation > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[[34042]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['Customer_ID'].isin(['CUS_0x6079'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are records which have very high annual income and monthly inhand salary is comparable but not same as other months. Rest of the data is same for other months and we can make it same after making them null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[[34042], ['Annual_Income', 'Monthly_Inhand_Salary']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Annual_Income'] = df.groupby('Customer_ID')['Annual_Income'].transform(forward_backward_fill)\n",
    "#df['Monthly_Inhand_Salary'] = df.groupby('Customer_ID')['Monthly_Inhand_Salary'].transform(forward_backward_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_annual_income = summarize_numerical_column_with_deviation(df, 'Annual_Income', 'Customer_ID', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_monthly_inhand_salary = summarize_numerical_column_with_deviation(df, 'Monthly_Inhand_Salary', 'Customer_ID', True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the distributions look ok now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Number of Bank Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_num_bank_accounts = summarize_numerical_column_with_deviation(df, 'Num_Bank_Accounts', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_num_bank_accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some negative values which are not possible. Lets convert them to null for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_Bank_Accounts'][df['Num_Bank_Accounts'] < 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max. MAD value is 1, this means MAD  can take values like 0, 0.5 or 1 given the nature of data. Lets try to observe the median standardization data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts')).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sort((df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts'))).unique())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four values look ok as median standardization should be usually small only. But after that the values look too big to be real and thus, can be changed to nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Num_Bank_Accounts'][df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Bank_Accounts')).abs() > 2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_num_bank_accounts = summarize_numerical_column_with_deviation(df, 'Num_Bank_Accounts', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the null values as an educated guess we can just take the forward and backward fills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Num_Bank_Accounts'] = df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(forward_backward_fill).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we take one level difference at customer level? The difference can be 0, negative or positive but shouldn't be too high as the number of bank accounts shouldnt usually jump or decrease too much suddenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Num_Bank_Accounts'].transform(pd.Series.diff).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly, the number of bank accounts remain same across months but rarely it can increase or decrease by one. This is understandable and looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Num_Bank_Accounts'].agg(lambda x: x.diff().sum()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of 1 and -1 are same as above. This is only possible when all these 1 and -1 belong to different customers i.e. in 8-months period either number of bank account remains same, increases by 1 or decreases by 1 in between the months for a customer. No weird patterns exist in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Number of credit cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same procedure as number of bank accounts can be followed for number of credit cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_num_credit_cards = summarize_numerical_column_with_deviation(df, 'Num_Credit_Card', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_num_credit_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max. MAD value is 1, this means MAD  can take values like 0, 0.5 or 1 given the nature of data. Lets try to observe the median standardization data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card')).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sort((df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card'))).unique())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four values look ok as median standardization should be usually small only. But after that the values look too big to be real and thus, can be changed to nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Num_Credit_Card'][df.groupby('Customer_ID')['Num_Credit_Card'].transform(median_standardization, default_value = return_max_MAD(df, 'Num_Credit_Card')).abs() > 2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_num_credit_cards = summarize_numerical_column_with_deviation(df, 'Num_Credit_Card', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the null values as an educated guess we can just take the forward and backward fills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_Credit_Card'] = df.groupby('Customer_ID')['Num_Credit_Card'].transform(forward_backward_fill).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we take one level difference at customer level? The difference can be 0, negative or positive but shouldn't be too high as the number of bank accountscredit cards shouldnt usually jump or decrease too much suddenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Num_Credit_Card'].transform(pd.Series.diff).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly, the number of credit cards remain same across months but rarely it can increase or decrease by one. This is understandable and looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Num_Credit_Card'].agg(lambda x: x.diff().sum()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of 1 and -1 are same as above. This is only possible when all these 1 and -1 belong to different customers i.e. in 8-months period either number of bank account remains same, increases by 1 or decreases by 1 in between somewhere. No weird patterns exist in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Interest Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_interest_rate = summarize_numerical_column_with_deviation(df, 'Interest_Rate', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_interest_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Interest_Rate'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we observe is MAD is 0(since max. MAD is 0) for each customer. Thus, it is hard to look at median standardization and assess points using this. Lets try to look at deviation from median. Since interest rate is not a feature whose median should deviate too much in scale from customer to customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviation_from_median = df.groupby('Customer_ID')['Interest_Rate'].transform(lambda x: (x - x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviation_from_median.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(deviation_from_median)\n",
    "# plt.title('Boxplot of interest rate deviation from customers median')\n",
    "# plt.ylabel('Deviation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deviation_from_median.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sort(deviation_from_median.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above data indicates that either interest rate is same as median or varies by at least 37% difference or more which is almost never seen in real life. We will fill all the records of customer with customer's median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Interest_Rate'] = df.groupby('Customer_ID')['Interest_Rate'].transform(lambda x: x.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Interest_Rate'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Number of loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_num_of_loans = summarize_numerical_column_with_deviation(df, 'Num_of_Loan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly some outliers as the number of loans can't be these many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of loans column can be used to extract this information accurately as the loans taken, their order and the count is embedded inside type of loan column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Type_of_Loan'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11408 null values inside type of loan column which were added by us after replacing the placeholder. Maybe these represent no loans. Lets extract the loans count from type of loan column and fill it in num of loans column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_loans = df['Type_of_Loan'].str.split(', ').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_of_Loan'][num_of_loans.notnull()] = num_of_loans[num_of_loans.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have changed the values of number of loans columns for which we had valid data from type of loan column. Lets look at the values in number of loans column which were not touched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_of_Loan'][num_of_loans.isnull()].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sort(df['Num_of_Loan'][num_of_loans.isnull()].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly these are 0 which represent no loans. Other than this there are either negative values or too high values to be representing real count for number of loans. Looking at the data, we can assume here that all these are erroneous values and should actually be 0 i.e. specifying no loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_of_Loan'][num_of_loans.isnull()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_of_Loan'] = df.groupby('Customer_ID')['Num_of_Loan'].transform(forward_backward_fill).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we take one level difference at customer level? The difference can be 0, negative or positive but shouldn't be too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Num_of_Loan'].transform(pd.Series.diff).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that number of loans remain same throughout the 8-months period for each customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Type of loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Type_of_Loan'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the placeholder 'Not Specified' has been used as a way of indicating that the type of loan has not been specified by the customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Type_of_Loan'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Type_of_Loan'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 11408 null values. As noted earlier with number of loans these most probably represent no loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace the same with our own placeholder for that - 'No Loan'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Type_of_Loan'].fillna('No Loan', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets seen what and how many unique type of loans we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_series = df['Type_of_Loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_lengths = temp_series.str.split(', ').str.len().astype(int) # Number of loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_lengths_max = temp_lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, val in temp_lengths.items():\n",
    "    # temp_series[index] = (temp_lengths_max - val) * 'No Loan, ' + temp_series[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = temp_series.str.split(pat = ', ', expand = True)\n",
    "# unique_loans = set()\n",
    "# for col in temp.columns:\n",
    "#     temp[col] = temp[col].str.lstrip('and ')\n",
    "#     unique_loans.update(temp[col].unique())\n",
    "# print(unique_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(unique_loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are total 8 unique type of loans, one placeholder for no specification of loan type and one placeholder added by us to specify there is no loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are working with tree based models usually they don't need the categorical columns to be encoded to numerical data type like we need for linear regression, logistic regression etc. as the model can handle these. But scikit learn uses CART algorithm and there is no functionality of using categorical variables directly and thus, for modelling with scikit-learn they need to be numerically encoded. Right now in this column we have 6260 unique categories which is too much to search for at one node. as at each node, a decision tree will look at all possible values of the categorical column to figure out what value produces the best split based on gini impurity or entropy decrease on splitting. We can do some pre-prcoessing on this column to split it into multiple columns. Intutively, it feels like the lastest loan should have high influence on your credit score because if the loan is heavy in nature then it might lead to poor credit score if unable to pay while if it is light then the credit score should remain almost same as before. We can split the column in following format: Latest loan1, latest loan2 etc. This way we will have 9 columns corresponding to maximum number of loans for any customer and each column can have maximum 10 categorical values corresponding to the unqiue loans calculated above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach will have following benefits:  \n",
    "1. Preserves the order of loans even after splitting.\n",
    "2. Easier to visualize and understand patterns since number of categories per column reduces.    \n",
    "3. Algorithm can focus more on information contained within the loan sequence. For example, if second last and third last loan contain critical information in classifying credit score than focussing on whole sequence of loans is not worthwhile and this inturn might lead to smaller decision trees and faster training compared to if we didn't split.  \n",
    "4. At each node for 9 of the splitted columns only total 90(9 columns * 10 categories) comparisons need to be made after one-hot encoding rather than 6260 comparisons for one non-splitted column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible approach could be to split columns as first loan, second loan etc. as a way of splitting but the pre-processing mentioned above feels more effective for now. We will maybe look at these two modeling startegies whn we do EDA and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.columns = [f'Last_Loan_{i}' for i in range(int(df['Num_of_Loan'].max()), 0, -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.merge(df, temp, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the type of loan column from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns = 'Type_of_Loan', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Delay from due date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_due_date = summarize_numerical_column_with_deviation(df, 'Delay_from_due_date', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_due_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median standardization varies quite a bit going from -10 to 11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due_date_deviation = df.groupby('Customer_ID')['Delay_from_due_date'].transform(median_standardization, default_value = return_max_MAD(df, 'Delay_from_due_date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the fact that overall distribution of delay from due date is not too extreme and delay from due date can vary a lot as well unlike number of credit cards or number of bank accounts. We will move forward with the data as it is. Having a domain expert by your side would have helped make this more clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Number of delayed payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_num_delayed_payments = summarize_numerical_column_with_deviation(df, 'Num_of_Delayed_Payment', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_num_delayed_payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from median standardization, almost all of the values are same as median and this is leading to 0 median standardization. Median standardization should definitely should be like this and should be skewed in nature but its hard to assess what threshold to use without a domain expert. We will use the full column as a sample and judge based on that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of delayed payments can't be too much and can not be negative as well. We will set negative values and values greater than upper range of oultiers to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_of_Delayed_Payment'][(df['Num_of_Delayed_Payment'] > summary_num_delayed_payments['Num_of_Delayed_Payment']['Outlier upper range']) | (df['Num_of_Delayed_Payment'] < 0)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_of_Delayed_Payment'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8382 null values. Lets observe the count of diff in between consecutive months and observe if we can identify some pattern exising there which can help us make some educated guess about the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(pd.Series.diff).value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 45.6% of time it remains same across months but rest of the time it varies i.e more than 50% of the time it varies across months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Customer_ID', 'Num_of_Delayed_Payment']].head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data it looks like usually a single value repeats more often across months i.e. mode might be a suitable choice here. But first lets see that usually how many times the mode occurs for any customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of frequency of mode and number of non-null data per customer\n",
    "# temp = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(lambda x: (x == x.mode()[0]).sum()/x.notnull().sum()).value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp[temp.index > 0.5].sum() # Idenitfying how many times the mode occurs in more than 50% of non-null data per customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is within given data for around 75.8% of the customers the mode occurs more than 50% of the time within 8-months period for whatever data we have available. This means the mode might be a suitable imputation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if there are multiple modes per customer? Lets check the data if such thing exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Num_of_Delayed_Payment'].agg(lambda x: len(x.mode())).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly, we observe one mode but sometimes it can be more than one as well. What to do in multiple modes case? We can take some average or median of modes in that case, in case there is skewness within the modes, median would be a better guess and in case where medians come out to be floating point number we can just take the integer part as an approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_of_Delayed_Payment'] = df.groupby('Customer_ID')['Num_of_Delayed_Payment'].transform(return_mode_median_filled_int).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of delayed payments is something that can vary from month to month its not like it has monotically increasing pattern or so which we can use as a sanity check. What we can try to observe is the relative devaition from median for this cleaned column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_numerical_column_with_deviation(df, 'Num_of_Delayed_Payment', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Changed credit limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_changed_credit_limit = summarize_numerical_column_with_deviation(df, 'Changed_Credit_Limit', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit card limit is dependent upon the users usage patterns. If the lender trusts the customer then it can increase also and if customer is late on payments, low activity etc. then the credit lmit can decrease as well. Thus, both negative and positive values are understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper range for outliers for full column doesn't significantly deviate from the max value and its difficult to judge here what threshold should be placed on credit limit median standardization. Thus, we leave non-null values as it is for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Customer_ID', 'Changed_Credit_Limit']].head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data usually the credit limit occurs with the same value across months i.e. the mode might be an appropriate value to imputate. Lets do some checks first though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Changed_Credit_Limit'].agg(lambda x: len(x.mode())).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the time only one mode appears. But sometimes two mode can occur as well, since this is a floating point type feature and there are only two mode values we will choose average of both which will be same as median in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Changed_Credit_Limit'] = df.groupby('Customer_ID')['Changed_Credit_Limit'].transform(return_mode_average_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Changed_Credit_Limit'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Number of credit card inquiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_num_credit_inquiries = summarize_numerical_column_with_deviation(df, 'Num_Credit_Inquiries', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from median standardization almost all of the values are same as median and this is leading to 0 median standardization. median standardization definitely should be like this and should be skewed in nature but its hard to assess what threshold to use without a domain expert. We will use the full column as a sample and judge based on that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of credit card inquiries cant be negative and can't be too extreme as well. We will replace those values with null for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_Credit_Inquiries'][(df['Num_Credit_Inquiries'] > summary_num_credit_inquiries['Num_Credit_Inquiries']['Outlier upper range']) | (df['Num_Credit_Inquiries'] < 0)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_Credit_Inquiries'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Customer_ID', 'Num_Credit_Inquiries']].head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a type of data which is monotically increasing in nature and thus should increase or remain same as months go on. Lets check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(pd.Series.diff).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected it mostly either remains same or increases. In this case we can just use forward fill and backward fill to fill these nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num_Credit_Inquiries'] = df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(forward_backward_fill).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Num_Credit_Inquiries'].transform(pd.Series.diff).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Credit Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit_mix_count = df['Credit_Mix'].value_counts()\n",
    "# sns.barplot(x = credit_mix_count.values, y = credit_mix_count.index, orient = 'h')\n",
    "# plt.xlabel('Count')\n",
    "# plt.ylabel('Credit Mix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Credit_Mix'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of null values present. We have already seen that during the 8-months period the type of loans and number of loans remain same so its fair to assume credit mix will also remain same. Lets check that with the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('Customer_ID')['Credit_Mix'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that one customer has only one type of credit mix only, throughout the 8-months period apart from null values. We can just use forward fill and bacward fill to achieve the desired goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Credit_Mix'] = df.groupby('Customer_ID')['Credit_Mix'].transform(forward_backward_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Credit_Mix'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Outstanding debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_outstanding_debt = summarize_numerical_column_with_deviation(df, 'Outstanding_Debt', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the values in median standardization are coming out to be zero. Is the outstanding debt constant for each customer across months after ignoring nulls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Outstanding_Debt'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column looks ok from the distribution perspective and there are no nulls present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Credit Utilization ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_credit_utilization_ratio = summarize_numerical_column_with_deviation(df, 'Credit_Utilization_Ratio', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from both the graphs its hard to put a threshold on median standardization of credit utilization ratio without a domin expert and also, the distribution of the column as a whole looks decent enough to not touch it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Credit History Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Customer_ID', 'Credit_History_Age']].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Credit_History_Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9030 null values. Looking at the data it is of the format - '{Year} Years and {Months} Months'. Using str functions of pandas series, we can extract these two data values i.e. year and months. We will then combine them in a single column as total months because both the year data and month data can be easily extracted from total months so there will be no loss of information and we will be able to reduce one feature from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Years', 'Months']] = df['Credit_History_Age'].str.extract('(?P<Years>\\d+) Years and (?P<Months>\\d+) Months').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Years', 'Months']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranges of both years and months look ok. We can proceed forward to combining them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Credit_History_Age'] = df['Years'] * 12 + df['Months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns = ['Years', 'Months'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to handle the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Credit_History_Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit history age is simply the numbr of months that the customer has been a customer and will just keep on increasing by one each month and thus, that logic can be used to fill the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Credit_History_Age'] = df.groupby('Customer_ID')['Credit_History_Age'].transform(fill_month_history).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. Payment of minimum amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Payment_of_Min_Amount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NM most probably means here that its not mentioned but in actual this is binary and it can contain either yes or no only. Lets see the data and figure out if we can replace it suitably with other values. First, lets check if each customer has same value for payment of minimum amount or it can vary across months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(['Customer_ID'])['Payment_of_Min_Amount'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be two possibilities here: one by replacing NM with mode of yes and no i.e. a customer is more likely to repeat his general behaviour or we can do forward fill and backward fill which would mean that current months decision is dependent upon previous month. Lets try to observe which is more probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we wil replace yes and no with 1 and 0 for sake of simplicity and NM with null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map({'Yes': 1, 'No': 0, 'NM': np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take difference across periods and observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Payment_of_Min_Amount'].transform(pd.Series.diff).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means either the value is same(with some nulls in between) throughout 8-months period for the customer or null occurs extactly at the transition from one value to another. So, if all values are same for a customer then we can fill the same otherwise null needs to be filled on transition and then we can fill with mode as the most probable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Payment_of_Min_Amount'] = df.groupby('Customer_ID')['Payment_of_Min_Amount'].transform(lambda x: x.fillna(x.mode()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Payment_of_Min_Amount'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map({1: 'Yes', 0: 'No'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. Total EMI per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_total_emi_per_month = summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_total_emi_per_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the median standardization, few points look out of the ordinary where relative deviation is greater than 10000. Having such high deviation is obviously very unlikely, we will first discard these few points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviation_total_emi = df.groupby('Customer_ID')['Total_EMI_per_month'].transform(median_standardization, default_value = return_max_MAD(df, 'Total_EMI_per_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Total_EMI_per_month'][deviation_total_emi > 10000] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_total_emi_per_month = summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its hard to quantify a threshold on median standardization without domain expertise and thats why we will use the whole column as a whole for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the whole column we will replace values more than upper range of outliers with null for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Total_EMI_per_month'][(df['Total_EMI_per_month'] > summary_total_emi_per_month['Total_EMI_per_month']['Outlier upper range'])] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Total_EMI_per_month'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6795 null values now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how many unique values of EMI exist per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Total_EMI_per_month'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 297 customers which don't have any EMI data at all. This is around 2.3% of the data. What does this mean? That 8-months period emi data is missing now but earlier the whole column did not have any null values at all then how did full 8-month data disappear for 297 customers? Since we did outlier detection by considering whole column as our sample maybe some emi's are real but still too high compared to majority of data and thus, got removed in the process or they are all erroneous, which we will judge later. This is where median standardization should help us but we don't have any means to set an appropriate threshold. Lets try to look at these customers whose data got removed completely before correction and correct any high deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviation_total_emi = df_copy.groupby('Customer_ID', group_keys = False)['Total_EMI_per_month'].transform(median_standardization, default_value = return_max_MAD(df_copy, 'Total_EMI_per_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = (deviation_total_emi[df.groupby('Customer_ID')['Total_EMI_per_month'].transform(pd.Series.nunique) == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(temp.values, orient = 'h')\n",
    "# plt.title('Median standardization of total EMI per month')\n",
    "# plt.xlabel('Median standardization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, only one value looks absurdly big. The rest of the median standardization's could even be considered ok for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp[temp > 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert this value to null and feed it back to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp[79370] = np.nan\n",
    "# df['Total_EMI_per_month'][temp.index] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_numerical_column_with_deviation(df, 'Total_EMI_per_month', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data looks more appropriate compared to before. There are still 4420 null values which need to be handled here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the null values can be filled using forward and backward fill as the EMI's should be highly dependent upon previous month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Total_EMI_per_month'] = df.groupby('Customer_ID')['Total_EMI_per_month'].transform(forward_backward_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Total_EMI_per_month'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19. Amount Invested Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_amount_invested_monthly = summarize_numerical_column_with_deviation(df, 'Amount_invested_monthly', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values in amount invested monthly are too extreme compared to the rest of the data and thus, can be removed considering them to be erroneous before we do further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Amount_invested_monthly'][df['Amount_invested_monthly'] > 8000] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the distribution again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_amount_invested_monthly = summarize_numerical_column_with_deviation(df, 'Amount_invested_monthly', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like power law distribution, hopefully these are not erroneous values. Lets leave these non-null values as it is for now. Null values still need to be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Amount_invested_monthly'].transform(return_num_of_modes).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets choose the median of values as a decent approximation for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Amount_invested_monthly'] = df.groupby('Customer_ID')['Amount_invested_monthly'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. Payment Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payment_behaviour_count = df['Payment_Behaviour'].value_counts()\n",
    "# sns.barplot(x = payment_behaviour_count.values, y = payment_behaviour_count.index, orient = 'h')\n",
    "# plt.xlabel('Count')\n",
    "# plt.ylabel('Payment Behaviour');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Payment_Behaviour'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7600 of null values present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Payment_Behaviour'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Payment_Behaviour'].agg(return_num_of_modes).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of modes vary, if the number of mode is 1 then we can use that for imputation else forward fill and backward fill can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Payment_Behaviour'] = df.groupby('Customer_ID')['Payment_Behaviour'].transform(lambda x: return_mode(x) if len(x.mode()) == 1 else forward_backward_fill(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Payment_Behaviour'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. Monthly Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_monthly_balance = summarize_numerical_column_with_deviation(df, 'Monthly_Balance', median_standardization_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1209 null values. Looking at the column as a whole the distribution looks ok and considering the fact that we can't decide exactly on a threshold on median standardization without domain expertise. We will leave the non-null values as it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('Customer_ID')['Monthly_Balance'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there might be skewness within the data we can use median to fill the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Monthly_Balance'] = df.groupby('Customer_ID')['Monthly_Balance'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Month column is not needed anymore and can be dropped. We will keep customer id as it is for now so that it can be used later on when doing train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns = ['Month'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac = 1) #shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearranging the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[:, ['Customer_ID', 'Age', 'Occupation', 'Annual_Income',\n",
    "#        'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
    "#        'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
    "#        'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n",
    "#        'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt',\n",
    "#        'Credit_Utilization_Ratio', 'Credit_History_Age',\n",
    "#        'Payment_of_Min_Amount', 'Total_EMI_per_month',\n",
    "#        'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance', 'Last_Loan_9', 'Last_Loan_8', 'Last_Loan_7',\n",
    "#        'Last_Loan_6', 'Last_Loan_5', 'Last_Loan_4', 'Last_Loan_3',\n",
    "#        'Last_Loan_2', 'Last_Loan_1',\n",
    "#        'Credit_Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output file path\n",
    "# file__out_path = os.path.join(parent_dir, \"data\\Credit_score_cleaned_data.csv\")\n",
    "# df.to_csv(file__out_path, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\\n\",\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct a path to the parent directory\\n\",\n",
    "\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Access a file in the parent directory\n",
    "\n",
    "file_path = os.path.join(parent_dir, \"data\\Credit_score_cleaned_data.csv\")\n",
    "\n",
    "# Load Credit Score data,\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Input For Neural Networks\n",
    "Drop or add columns that for the model.\n",
    "This is where you control your input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dropping columns not in used in this model\n",
    "##############################\n",
    "# columns_to_drop_not_used= ['Num_Bank_Accounts', 'Num_of_Loan', 'Last_Loan_9','Last_Loan_8', 'Last_Loan_7', 'Last_Loan_6'\n",
    "#                            ,'Last_Loan_5', 'Last_Loan_4', 'Last_Loan_3','Last_Loan_2', 'Last_Loan_1', 'Delay_from_due_date'\n",
    "#                            , 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Outstanding_Debt', 'Payment_of_Min_Amount'\n",
    "#                            , 'Num_Credit_Inquiries', 'Payment_of_Min_Amount', 'Total_EMI_per_month', 'Amount_invested_monthly'\n",
    "#                            , 'Credit_History_Age','Monthly_Balance', 'Payment_Behaviour']\n",
    "\n",
    "# 'Type_of_Loan' was replaced with \n",
    "# 'Last_Loan_9','Last_Loan_8','Last_Loan_7','Last_Loan_6','Last_Loan_5','Last_Loan_4','Last_Loan_3','Last_Loan_2','Last_Loan_1'\n",
    "\n",
    "# Drop columns\n",
    "# df.drop(columns=columns_to_drop_not_used, inplace=True)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jesse 11/21/2024: I don't think this cell is needed. It causes errors because of the new column names from the \n",
    "# # clean data. The results don't seem to change with this commented out.\n",
    "\n",
    "\n",
    "# # Drop costumer ID and cast ID\n",
    "# # df = df.drop(columns='Customer_ID')\n",
    "\n",
    "# # df['ID'] = df['ID'].astype('string')\n",
    "\n",
    "# # Getting information on the data frame\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to change the variables for your model, do that here!\n",
    "target = ['Credit_Score']\n",
    "continuous_features = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Credit_Card', 'Interest_Rate', 'Credit_Utilization_Ratio', 'Credit_History_Age','Monthly_Balance', 'Num_Credit_Inquiries', 'Outstanding_Debt', 'Num_of_Delayed_Payment', 'Delay_from_due_date', 'Changed_Credit_Limit', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Num_Bank_Accounts', 'Num_of_Loan'] \n",
    "categorical_features = ['Credit_Mix', 'Payment_Behaviour','Last_Loan_5', 'Last_Loan_4', 'Last_Loan_3','Last_Loan_2', 'Last_Loan_1', 'Payment_of_Min_Amount', 'Payment_of_Min_Amount', 'Last_Loan_9','Last_Loan_8', 'Last_Loan_7', 'Last_Loan_6', 'Occupation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode variables to use in Neural Network\n",
    "# # A one hot encoding is appropriate for categorical data where no relationship exists between categories.\n",
    "# # It involves representing each categorical variable with a binary vector that has one element for each \n",
    "# # unique label and marking the class label with a 1 and all other elements 0.\n",
    "\n",
    "# # For example, if our variable was “color” and the labels were “red,” “green,” and “blue,” we would encode \n",
    "# # each of these labels as a three-element binary vector as follows:\n",
    "\n",
    "# # Red: [1, 0, 0]\n",
    "# # Green: [0, 1, 0]\n",
    "# # Blue: [0, 0, 1]\n",
    "\n",
    "# # Then each label in the dataset would be replaced with a vector (one column becomes three). \n",
    "# # This is done for all categorical variables so that our nine input variables or columns become \n",
    "# # 43 in the case of the breast cancer dataset.\n",
    "\n",
    "# # The scikit-learn library provides the OneHotEncoder to automatically one hot encode one or more variables.\n",
    "\n",
    "# # Encoder for input features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# # Encoder for target\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoding categorical features\n",
    "encoded_features = encoder.fit_transform(df[categorical_features])\n",
    "\n",
    "# # Convert the encoded data back to a DataFrame:\n",
    "encoded_df = pd.DataFrame(encoded_features.toarray(), columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# # joining dataframes \n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoding categorical features\n",
    "encoded_target = encoder.fit_transform(df[target])\n",
    "\n",
    "# # Convert the encoded data back to a DataFrame:\n",
    "encoded_target_df = pd.DataFrame(encoded_target.toarray(), columns=encoder.get_feature_names_out(target))\n",
    "\n",
    "# # joining dataframes \n",
    "df = pd.concat([df, encoded_target_df], axis=1)\n",
    "# print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing dataframe for modeling\n",
    "features_for_model = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Credit_Card'\n",
    "                          , 'Interest_Rate', 'Credit_Utilization_Ratio', 'Credit_Mix_Bad'\n",
    "                          , 'Credit_Mix_Good', 'Credit_Mix_Standard', 'Last_Loan_3','Last_Loan_2', 'Last_Loan_1'\n",
    "                          , 'Credit_History_Age','Monthly_Balance', 'Payment_Behaviour'\n",
    "                          , 'Num_Credit_Inquiries','Last_Loan_5', 'Last_Loan_4', 'Outstanding_Debt', 'Num_of_Delayed_Payment'\n",
    "                          , 'Delay_from_due_date', 'Payment_of_Min_Amount', 'Changed_Credit_Limit', 'Total_EMI_per_month'\n",
    "                          , 'Amount_invested_monthly', 'Num_Bank_Accounts', 'Num_of_Loan', 'Last_Loan_9','Last_Loan_8', 'Last_Loan_7', 'Last_Loan_6'\n",
    "                          , 'Occupation_Accountant', 'Occupation_Architect'\n",
    "                          , 'Occupation_Developer', 'Occupation_Doctor', 'Occupation_Engineer', 'Occupation_Entrepreneur'\n",
    "                          , 'Occupation_Journalist', 'Occupation_Lawyer', 'Occupation_Manager', 'Occupation_Mechanic'\n",
    "                          ,'Occupation_Media_Manager' , 'Occupation_Musician', 'Occupation_Scientist', 'Occupation_Teacher'\n",
    "                          , 'Occupation_Writer'] \n",
    "\n",
    "target_features = ['Credit_Score_Good', 'Credit_Score_Poor', 'Credit_Score_Standard']\n",
    "\n",
    "# # Getting the size of input size\n",
    "# print(len(features_for_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining data sets\n",
    "X = encoded_features.toarray()\n",
    "y = encoded_target.toarray()\n",
    "# # y = to_categorical(df[target_features])\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic train-test split\n",
    "# # 80% training and 20% test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.20, random_state=42)\n",
    "\n",
    "# # Checking the dimensions of the variables\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing X_train and y_train\n",
    "# print(X_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the layers\n",
    "# ###################\n",
    "# # The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n",
    "# # Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are learned during training.\n",
    "\n",
    "# # Create network topology\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Adding input model --> 24 input layers\n",
    "model.add(Dense(46, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "\n",
    "# Adding hidden layer \n",
    "model.add(keras.layers.Dense(1000, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(512, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "# # output layer\n",
    "# # For classification tasks, we generally tend to add an activation function in the output (\"sigmoid\" for binary, and \"softmax\" for multi-class, etc.).\n",
    "model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the Model\n",
    "# ###################\n",
    "# # Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "# # Loss function —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "# # Optimizer —This is how the model is updated based on the data it sees and its loss function.\n",
    "# # Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n",
    "\n",
    "# # compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the Model\n",
    "# # ################\n",
    "# # Training the neural network model requires the following steps:\n",
    "# # Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays.\n",
    "# # The model learns to associate images and labels.\n",
    "# # You ask the model to make predictions about a test set—in this example, the test_images array.\n",
    "# # Verify that the predictions match the labels from the test_labels array.\n",
    "# # Feed the model\n",
    "# # To start training, call the model.fit method—so called because it \"fits\" the model to the training data:\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 30, batch_size = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "# print('\\nTest accuracy:', test_acc)\n",
    "# print('\\nLoss:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions():\n",
    "    global X_train, X_test, y_test, model, encoder  \n",
    "    # Check if required variables are initialized\n",
    "    if 'X_train' not in globals() or 'X_test' not in globals() or 'y_test' not in globals():\n",
    "        print(\"Error: Train-test split has not been performed. Please train the model first.\")\n",
    "        return\n",
    "\n",
    "    print(\"Generate Predictions:\")\n",
    "    print(\"********************\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate predictions\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Generating prediction using selected Neural Network\")\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Logging the size of datasets\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Size of training set: {X_train.shape[0]}\")\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Size of testing set: {X_test.shape[0]}\")\n",
    "\n",
    "    # Decode predictions and true labels\n",
    "    y_tested = y_test.argmax(axis=1)  # For multi-class classification\n",
    "    y_predicted = predictions.argmax(axis=1)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"True Value\": y_tested,\n",
    "        \"Predicted Value\": y_predicted\n",
    "    })\n",
    "    # predictions_df.to_csv(\"predictions.csv\", index=False)\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    file__out_path = os.path.join(parent_dir, \"data\\predictions.csv\")\n",
    "    predictions_df.to_csv(file__out_path, index = False)\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Predictions generated (predictions.csv have been generated)....\")\n",
    "\n",
    "    # Display some predictions\n",
    "    data = []\n",
    "    for i in range(15):\n",
    "        data.append([y_tested[i], y_predicted[i]])\n",
    "\n",
    "    headers = [\"True Value\", \"Predicted Value\"]\n",
    "    # print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    class_labels = ['Good', 'Poor', 'Standard']\n",
    "    # plot_prediction_vs_test_categorical(y_tested, y_predicted, class_labels)\n",
    "\n",
    "    # Calculate model performance\n",
    "    calculate_performance_multiclass(y_tested, y_predicted)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTime to process: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make Predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Here, the model has predicted the label for each image in the testing set. Let's take a look at some predictions\n",
    "# print(predictions[0])\n",
    "# print(predictions[10])\n",
    "# print(predictions[100])\n",
    "# print(predictions[1000])\n",
    "# print(predictions[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3 different credit scores. You can see the comparison between the trained and tested values\n",
    "\n",
    "# # getting y_test values\n",
    "# y_tested = encoder.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "# # getting the value of the predictions\n",
    "# y_predicted = encoder.inverse_transform(predictions)\n",
    "\n",
    "# # printing the first 15 values of the test and predicted values \n",
    "# data = []\n",
    "# for i in range(15):\n",
    "#     data.append([y_tested[i], y_predicted[i]])\n",
    "\n",
    "# headers = [\"True Value\", \"Predicted Value\"]\n",
    "\n",
    "# print(tabulate(data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix\n",
    "# ##################\n",
    "\n",
    "# # A confusion matrix for 3 variables is a table that visually represents how well a classification model performs when \n",
    "# # predicting three different categories, where each row represents the actual class and each column represents the predicted class,\n",
    "# # resulting in a 3x3 grid that shows how many instances were correctly classified and how many were misclassified between each \n",
    "# # of the three possible categories; essentially, it provides a detailed breakdown of the model's errors for each class in \n",
    "# # a multi-class classification problem.\n",
    "\n",
    "# # Key points about a 3-variable confusion matrix:\n",
    "# ################################################\n",
    "# # Structure:\n",
    "# # The matrix has 3 rows and 3 columns, where each row represents one of the actual classes and each column represents one of the predicted classes. \n",
    " \n",
    "# # Diagonal elements:\n",
    "# # The diagonal cells of the matrix represent the correctly classified instances for each class. \n",
    " \n",
    "# # Off-diagonal elements:\n",
    "# # The values in off-diagonal cells represent the misclassified instances, showing which class the model tends to confuse with another. \n",
    "\n",
    "# # Class labels\n",
    "# class_labels=['Good', 'Poor', 'Standard']\n",
    "\n",
    "# plot_prediction_vs_test_categorical(y_tested, y_predicted, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explanation of Metrics\n",
    "# ########################\n",
    "\n",
    "# # Accuracy: The proportion of correctly classified samples.\n",
    "# # Precision: The ability of the classifier not to label a negative sample as positive.\n",
    "# # Recall: The ability of the classifier to find all the positive samples.\n",
    "# # F1-score: A weighted average of precision and recall.\n",
    "# # Confusion Matrix: A table showing the number of true positives, true negatives, false positives, and false negatives for each class. \n",
    " \n",
    "# # Important Considerations:\n",
    "# # Averaging:\n",
    "# # The average parameter in precision_score, recall_score, and f1_score can be set to different values:\n",
    "# # 'macro': Calculates the metric for each label, and finds their unweighted mean.\n",
    "# # 'micro': Calculates the metric globally by counting the total true positives, false negatives, and false positives.\n",
    "# # 'weighted': Calculates the metric for each label, and finds their average weighted by support (the number of true instances for each label).\n",
    "# # Class Imbalances:\n",
    "# # If your dataset has class imbalances, consider using metrics like f1_score and recall that are less sensitive to this issue.\n",
    "\n",
    "# # Calculating perofrmace of model\n",
    "# calculate_performance_multiclass(y_tested, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainMenu():\n",
    "    global df, X_test, y_test, model, encoder\n",
    "    while True:\n",
    "        print(\"\\n===============================================\")\n",
    "        print(\"Menu:\")\n",
    "        print(\"(1) Load data\")\n",
    "        print(\"(2) Process (Clean) data\")\n",
    "        print(\"(3) Train NN\")\n",
    "        print(\"(4) Generate Predictions\")\n",
    "        print(\"(5) Quit\")\n",
    "        print(\"===============================================\")\n",
    "        \n",
    "        choice = input(\"Select Option: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            loadData()\n",
    "        \n",
    "        elif choice == '2':\n",
    "            dataCleaning()\n",
    "                \n",
    "        elif choice == '3':\n",
    "            trainNN()\n",
    "        \n",
    "        elif choice == '4':\n",
    "            predictions()\n",
    "        \n",
    "        elif choice == '5':\n",
    "            print(\"Exiting the program.\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid option. Please select a valid option.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainMenu()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
